Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Affiliations,Authors with affiliations,Abstract,Author Keywords,Index Keywords,Correspondence Address,Editors,Publisher,ISSN,ISBN,CODEN,PubMed ID,Language of Original Document,Abbreviated Source Title,Document Type,Publication Stage,Open Access,Source,EID
"Mabina P., Mukoma P., Booysen M.J.","57224176058;8677820200;51664541800;","Sustainability matchmaking: Linking renewable sources to electric water heating through machine learning",2021,"Energy and Buildings","246",, 111085,"","",,,"10.1016/j.enbuild.2021.111085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107143661&doi=10.1016%2fj.enbuild.2021.111085&partnerID=40&md5=26938e8d66056f7aabc52a27ed931f5a","Department of E&E Engineering, Stellenbosch University, South Africa; Energy Centre, Council for Scientific and Industrial Research, South Africa","Mabina, P., Department of E&E Engineering, Stellenbosch University, South Africa, Energy Centre, Council for Scientific and Industrial Research, South Africa; Mukoma, P., Energy Centre, Council for Scientific and Industrial Research, South Africa; Booysen, M.J., Department of E&E Engineering, Stellenbosch University, South Africa","A high penetration of renewable energy sources such as wind power generation and photovoltaic generation causes some problems in power systems such as the duck curve and unreliability due to environmental variability. An effective solution to this problem is Demand Response (DR). Electric Water Heaters (EWHs) are considered ideal candidates for DR due to their energy storage capability. Due to the benefits, control strategies or techniques for EWHs have received considerable academic attention. The energy sector has recently tapped into the disruptive artificial intelligence world to learn, among other related priorities, how to enhance operations, maintain energy resilience and improve consumer service. Consequently, this paper reviews the use of machine learning (ML) for optimization and scheduling of EWHs. The main contributions of this review paper are, firstly, to identify state of the art of energy optimization and scheduling of EWHs. Secondly, to review the current ML models for energy optimization and scheduling of EWHs in smart grids and smart building environment. While classical control strategies may deliver substantial improvements, optimum efficiency may not be reached. ML has demonstrated clear advantages over classical control. Based on these conclusions, recommendations for further research topics are drawn. © 2021 Elsevier B.V.","Electric water heaters; Machine learning; Reinforcement learning; Renewable energy; Supervised learning; Unsupervised learning",,"Booysen, M.J.; Department of E&E Engineering, South Africa",,"Elsevier Ltd",03787788,,ENEBD,,"English","Energy Build.",Review,"Final","All Open Access, Green",Scopus,2-s2.0-85107143661
"Dong B., Liu Y., Fontenot H., Ouf M., Osman M., Chong A., Qin S., Salim F., Xue H., Yan D., Jin Y., Han M., Zhang X., Azar E., Carlucci S.","35368332400;57218531312;57203941191;56422210800;57222487522;57189900965;57222999016;7801420000;57223016278;7401863008;57199284864;55345227200;57209625490;36975172200;55331189400;","Occupant behavior modeling methods for resilient building design, operation and policy at urban scale: A review",2021,"Applied Energy","293",, 116856,"","",,,"10.1016/j.apenergy.2021.116856","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104462832&doi=10.1016%2fj.apenergy.2021.116856&partnerID=40&md5=5d42814570996891cd9bb46917cbcba8","Department of Mechanical & Aerospace Engineering, Syracuse University, 263 Link Hall, Syracuse, NY  13244, United States; Department of Building, Civil and Environmental Engineering, Concordia University, 1455 De Maisonneuve Blvd. West, EV 6.139, Montreal, QC  H3G 1M8, Canada; Department of Building, School of Design and Environment, National University of Singapore, 4 Architecture Drive, Singapore, 117566, Singapore; School of Computing Technologies, RMIT University, Melbourne, Victoria  3000, Australia; Building Energy Research Center, School of Architecture, Tsinghua University, Beijing, 100084, China; School of Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Department of Industrial and Systems Engineering, Khalifa University of Science and Technology, PO Box 127788, Abu Dhabi, United Arab Emirates; Energy, Environment, And Water Research Center, The Cyprus Institute, Nicosia, Cyprus","Dong, B., Department of Mechanical & Aerospace Engineering, Syracuse University, 263 Link Hall, Syracuse, NY  13244, United States; Liu, Y., Department of Mechanical & Aerospace Engineering, Syracuse University, 263 Link Hall, Syracuse, NY  13244, United States; Fontenot, H., Department of Mechanical & Aerospace Engineering, Syracuse University, 263 Link Hall, Syracuse, NY  13244, United States; Ouf, M., Department of Building, Civil and Environmental Engineering, Concordia University, 1455 De Maisonneuve Blvd. West, EV 6.139, Montreal, QC  H3G 1M8, Canada; Osman, M., Department of Building, Civil and Environmental Engineering, Concordia University, 1455 De Maisonneuve Blvd. West, EV 6.139, Montreal, QC  H3G 1M8, Canada; Chong, A., Department of Building, School of Design and Environment, National University of Singapore, 4 Architecture Drive, Singapore, 117566, Singapore; Qin, S., Department of Building, School of Design and Environment, National University of Singapore, 4 Architecture Drive, Singapore, 117566, Singapore; Salim, F., School of Computing Technologies, RMIT University, Melbourne, Victoria  3000, Australia; Xue, H., School of Computing Technologies, RMIT University, Melbourne, Victoria  3000, Australia; Yan, D., Building Energy Research Center, School of Architecture, Tsinghua University, Beijing, 100084, China; Jin, Y., Building Energy Research Center, School of Architecture, Tsinghua University, Beijing, 100084, China; Han, M., School of Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Zhang, X., School of Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Azar, E., Department of Industrial and Systems Engineering, Khalifa University of Science and Technology, PO Box 127788, Abu Dhabi, United Arab Emirates; Carlucci, S., Energy, Environment, And Water Research Center, The Cyprus Institute, Nicosia, Cyprus","Traditional occupant behavior modeling has been studied at the building level, and it has become an important factor in the investigation of building energy consumption. However, studies modeling occupant behaviors at the urban scale are still limited. Recent work has revealed that urban big data can enable occupant behavior modeling at the urban scale – however, utilizing the existing data sources and modeling methods in building science to model urban scale occupant behaviors can be quite challenging. Beyond building science, urban scale human behaviors have been studied in several different domains using more advanced modeling methods, including Stochastic Modeling, Neural Networks, Reinforcement Learning, Network Modeling, etc. This paper aims to bridge the gap between data sources and modeling methodologies in building science by borrowing from other domains. Based on a comprehensive review, we 1) identify the modeling challenges of the current approaches in building science, 2) discuss the modeling requirements and data sources both in building science and other domains, 3) review the current modeling methods in building science and other domains, and 4) summarize available performance evaluation metrics for evaluating the modeling methods. Finally, we present future opportunities in building science with enhanced data sources and modeling methods from other domains. © 2021 Elsevier Ltd","Building science; Cross domain; Human mobility modeling; Occupant behavior modeling; Performance evaluation; Spatio-temporal data; Urban scale","Architectural design; Behavioral research; Reinforcement learning; Stochastic systems; Structural design; Building science; Cross-domain; Human mobility modeling; In-buildings; Model method; Occupant behavior modeling; Occupants behaviours; Performances evaluation; Spatio-temporal data; Urban scale; Energy utilization; architectural design; energy use; human behavior; literature review; numerical model; operations technology; performance assessment; stochasticity; urban policy","Dong, B.; Department of Mechanical & Aerospace Engineering, 263 Link Hall, United States; email: bidong@syr.edu",,"Elsevier Ltd",03062619,,APEND,,"English","Appl. Energy",Article,"Final","",Scopus,2-s2.0-85104462832
"Ibarz J., Tan J., Finn C., Kalakrishnan M., Pastor P., Levine S.","57191410963;42262817800;55938427500;24467757900;57197413733;35731728100;","How to train your robot with deep reinforcement learning: lessons we have learned",2021,"International Journal of Robotics Research","40","4-5",,"698","721",,4,"10.1177/0278364920987859","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100589220&doi=10.1177%2f0278364920987859&partnerID=40&md5=0dafd0815b4dd66b6cdacdb91b8630de","Robotics at Google, Mountain View, CA, United States; Stanford University, Stanford, CA, United States; The Moonshot Factory, Mountain View, CA, United States; University of California Berkeley, Berkeley, CA, United States","Ibarz, J., Robotics at Google, Mountain View, CA, United States; Tan, J., Robotics at Google, Mountain View, CA, United States; Finn, C., Robotics at Google, Mountain View, CA, United States, Stanford University, Stanford, CA, United States; Kalakrishnan, M., The Moonshot Factory, Mountain View, CA, United States; Pastor, P., The Moonshot Factory, Mountain View, CA, United States; Levine, S., Robotics at Google, Mountain View, CA, United States, University of California Berkeley, Berkeley, CA, United States","Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low-level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world. At the same time, real-world robotics provides an appealing domain for evaluating such algorithms, as it connects directly to how humans learn: as an embodied agent in the real world. Learning to perceive and move in the real world presents numerous challenges, some of which are easier to address than others, and some of which are often not considered in RL research that focuses only on simulated domains. In this review article, we present a number of case studies involving robotic deep RL. Building off of these case studies, we discuss commonly perceived challenges in deep RL and how they have been addressed in these works. We also provide an overview of other outstanding challenges, many of which are unique to the real-world robotics setting and are not often the focus of mainstream RL research. Our goal is to provide a resource both for roboticists and machine learning researchers who are interested in furthering the progress of deep RL in the real world. © The Author(s) 2021.","deep learning; reinforcement learning; Robotics","Agricultural robots; Reinforcement learning; Robotics; Robots; Case-studies; Embodied agent; Level sensors; Physical robots; Real environments; Real-world; Simulated domains; Video game; Deep learning","Ibarz, J.; Robotics at GoogleUnited States; email: julianibarz@google.com",,"SAGE Publications Inc.",02783649,,IJRRE,,"English","Int J Rob Res",Article,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85100589220
"Abhishek, Dhankar A., Gupta N.","57212317597;57200942175;57189757459;","A systematic review of techniques, tools and applications of machine learning",2021,"Proceedings of the 3rd International Conference on Intelligent Communication Technologies and Virtual Mobile Networks, ICICV 2021",,, 9388637,"764","768",,,"10.1109/ICICV50876.2021.9388637","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104390148&doi=10.1109%2fICICV50876.2021.9388637&partnerID=40&md5=94c6ae2939bf85e16867cc0ed8a13701","Manav Rachna International Institute of Research and Studies, Faridabad, India","Abhishek, Manav Rachna International Institute of Research and Studies, Faridabad, India; Dhankar, A., Manav Rachna International Institute of Research and Studies, Faridabad, India; Gupta, N., Manav Rachna International Institute of Research and Studies, Faridabad, India","Machine Learning is a part of Artificial Intelligence. A branch of artificial Intelligence(AI), that offers the capability to the system by learning on their own and work better from experience without human intervention. By using machine learning (ML), software applications become more precise for outcome prediction with no requirement of explicit programmed. Machine learning has a fundamental concept i.e. building of algorithms for accepting input data and with the help of statistical analysis they can predict the output. Nowadays, machine learning has gained vast popularity, and its algorithms are used in every field like object detection, pattern recognition, text interpretation, and different research areas. One of the basic goals of machine learning is to teach computers to use the data to solve a particular problem. A fair range of machine-learning applications include e-mail classification training to distinguish spam from non-spam messages, fraud detection, etc. This paper aims at varied machine learning methods, algorithms, and tools required to run the machine learning projects. It also focuses on the advancements that are administered in order so that the current researchers can be benefited out of it. © 2021 IEEE.","Clustering; Machine Learning; Reinforcement Learning; Supervised Learning; Unsupervised Learning","Application programs; Cellular radio systems; Character recognition; Electronic mail; Learning algorithms; Mobile telecommunication systems; Object detection; Wireless networks; Email classification; Fundamental concepts; Human intervention; Machine learning applications; Machine learning methods; Outcome prediction; Software applications; Tools and applications; Machine learning",,,"Institute of Electrical and Electronics Engineers Inc.",,9780738111834,,,"English","Proc. Int. Conf. Intell. Commun. Technol. Virtual Mob. Networks, ICICV",Conference Paper,"Final","",Scopus,2-s2.0-85104390148
"Mouchlis V.D., Afantitis A., Serra A., Fratello M., Papadiamantis A.G., Aidinis V., Lynch I., Greco D., Melagraki G.","36463891800;6507913243;56670333700;55702179300;57196452586;6602795121;6603538848;56394030300;8283563600;","Advances in de novo drug design: From conventional to machine learning methods",2021,"International Journal of Molecular Sciences","22","4", 1676,"1","22",,2,"10.3390/ijms22041676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100512934&doi=10.3390%2fijms22041676&partnerID=40&md5=53f367367944e5ef3becbbddf7008a44","Department of ChemoInformatics, NovaMechanics Ltd., Nicosia, 1046, Cyprus; Faculty of Medicine and Health Technology, Tampere University, Tampere, 33520, Finland; BioMEdiTech Institute, Tampere University, Tampere, 33520, Finland; School of Geography, Earth and Environmental Sciences, University of Birmingham, Birmingham, B15 2TT, United Kingdom; Institute for Bioinnovation, Biomedical Sciences Research Center Alexander Fleming, Fleming 34, Athens, 16672, Greece; Institute of Biotechnology, University of Helsinki, Helsinki, 00014, Finland; Finnish Center for Alternative Methods (FICAM), Tampere University, Tampere, 33520, Finland; Division of Physical Sciences & Applications, Hellenic Military Academy, Vari, 16672, Greece","Mouchlis, V.D., Department of ChemoInformatics, NovaMechanics Ltd., Nicosia, 1046, Cyprus; Afantitis, A., Department of ChemoInformatics, NovaMechanics Ltd., Nicosia, 1046, Cyprus; Serra, A., Faculty of Medicine and Health Technology, Tampere University, Tampere, 33520, Finland, BioMEdiTech Institute, Tampere University, Tampere, 33520, Finland; Fratello, M., Faculty of Medicine and Health Technology, Tampere University, Tampere, 33520, Finland, BioMEdiTech Institute, Tampere University, Tampere, 33520, Finland; Papadiamantis, A.G., Department of ChemoInformatics, NovaMechanics Ltd., Nicosia, 1046, Cyprus, School of Geography, Earth and Environmental Sciences, University of Birmingham, Birmingham, B15 2TT, United Kingdom; Aidinis, V., Institute for Bioinnovation, Biomedical Sciences Research Center Alexander Fleming, Fleming 34, Athens, 16672, Greece; Lynch, I., School of Geography, Earth and Environmental Sciences, University of Birmingham, Birmingham, B15 2TT, United Kingdom; Greco, D., Faculty of Medicine and Health Technology, Tampere University, Tampere, 33520, Finland, BioMEdiTech Institute, Tampere University, Tampere, 33520, Finland, Institute of Biotechnology, University of Helsinki, Helsinki, 00014, Finland, Finnish Center for Alternative Methods (FICAM), Tampere University, Tampere, 33520, Finland; Melagraki, G., Division of Physical Sciences & Applications, Hellenic Military Academy, Vari, 16672, Greece","De novo drug design is a computational approach that generates novel molecular structures from atomic building blocks with no a priori relationships. Conventional methods include structure‐based and ligand‐based design, which depend on the properties of the active site of a biological target or its known active binders, respectively. Artificial intelligence, including machine learning, is an emerging field that has positively impacted the drug discovery process. Deep reinforcement learning is a subdivision of machine learning that combines artificial neural networks with reinforcement‐learning architectures. This method has successfully been employed to develop novel de novo drug design approaches using a variety of artificial networks including recurrent neural networks, convolutional neural networks, generative adversarial networks, and autoencod-ers. This review article summarizes advances in de novo drug design, from conventional growth algorithms to advanced machine‐learning methodologies and highlights hot topics for further de-velopment. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Artificial intelligence; Artificial neural networks; Autoencoders; Convolutional neural networks; De novo drug design; Deep reinforcement learning; Generative adversarial networks; Machine learning; Recurrent neural networks","ligand; drug; adversarial autoencoder; artificial intelligence; artificial neural network; autoencoder; biodiversity; clinical evaluation; convolutional neural network; deep learning; deep reinforcement learning; drug design; drug development; drug structure; evolutionary algorithm; generative adversarial network; human; machine learning; molecular biology; molecular clock; particle swarm optimization; program feasibility; recurrent neural network; reinforcement learning (machine learning); Review; sampling; sequence to sequence autoencoder; toxicogenomics; variational autoencoder; animal; chemistry; Animals; Drug Design; Humans; Machine Learning; Neural Networks, Computer; Pharmaceutical Preparations","Mouchlis, V.D.; Department of ChemoInformatics, Cyprus; email: mouchlis@novamechanics.com
Afantitis, A.; Department of ChemoInformatics, Cyprus; email: afantitis@novamechanics.com
Melagraki, G.; Division of Physical Sciences & Applications, Greece; email: georgiamelagraki@gmail.com",,"MDPI AG",16616596,,,"33562347","English","Int. J. Mol. Sci.",Review,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85100512934
"Lamata L.","57203115348;","Quantum reinforcement learning with quantum photonics",2021,"Photonics","8","2", 33,"1","8",,1,"10.3390/photonics8020033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100617962&doi=10.3390%2fphotonics8020033&partnerID=40&md5=d666519c68264c81bc1afeb3ca5a6ec1","Departamento de Física Atómica, Molecular y Nuclear, Universidad de Sevilla, Apartado de Correos 1065, Sevilla, 41080, Spain","Lamata, L., Departamento de Física Atómica, Molecular y Nuclear, Universidad de Sevilla, Apartado de Correos 1065, Sevilla, 41080, Spain","Quantum machine learning has emerged as a promising paradigm that could accelerate machine learning calculations. Inside this field, quantum reinforcement learning aims at designing and building quantum agents that may exchange information with their environment and adapt to it, with the aim of achieving some goal. Different quantum platforms have been considered for quantum machine learning and specifically for quantum reinforcement learning. Here, we review the field of quantum reinforcement learning and its implementation with quantum photonics. This quantum technology may enhance quantum computation and communication, as well as machine learning, via the fruitful marriage between these previously unrelated fields. © 2021 by the author. Licensee MDPI, Basel, Switzerland.","Quantum communication; Quantum machine learning; Quantum photonics; Quantum reinforcement learning; Quantum technologies",,"Lamata, L.; Departamento de Física Atómica, Apartado de Correos 1065, Spain; email: llamata@us.es",,"MDPI AG",23046732,,,,"English","Photonics",Review,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85100617962
"Hu R., Zhang S., Meng H., Yu H., Zhang J., Luo X., Si T., Liu C., Qiao Y.","57222085750;55264974500;57205669471;57218552887;57218253626;57222098325;54946193100;8559807600;36086392600;","Machine learning for synthetic biology: Methods and applications [面向合成生物学的机器学习方法及应用]",2021,"Kexue Tongbao/Chinese Science Bulletin","66","3",,"284","299",,2,"10.1360/TB-2020-0456","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101332265&doi=10.1360%2fTB-2020-0456&partnerID=40&md5=edf5ba3c8b2b0b8887c2a672ebafef5b","Institute of Advanced Computing and Digital Engineering, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China; Institute of Synthetic Biology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China; Shenzhen Institute of Synthetic Biology, Shenzhen, 518055, China; CAS Key Laboratory of Quantitative Engineering Biology, Shenzhen, 518055, China; Center for Biological Engineering, Guangzhou Institute of Advanced Technology, Chinese Academy of Sciences, Guangzhou, 511458, China","Hu, R., Institute of Advanced Computing and Digital Engineering, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China; Zhang, S., Institute of Synthetic Biology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China, Shenzhen Institute of Synthetic Biology, Shenzhen, 518055, China, CAS Key Laboratory of Quantitative Engineering Biology, Shenzhen, 518055, China; Meng, H., Institute of Synthetic Biology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China, Center for Biological Engineering, Guangzhou Institute of Advanced Technology, Chinese Academy of Sciences, Guangzhou, 511458, China; Yu, H., Institute of Synthetic Biology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China, Shenzhen Institute of Synthetic Biology, Shenzhen, 518055, China, CAS Key Laboratory of Quantitative Engineering Biology, Shenzhen, 518055, China; Zhang, J., Institute of Synthetic Biology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China, Shenzhen Institute of Synthetic Biology, Shenzhen, 518055, China, CAS Key Laboratory of Quantitative Engineering Biology, Shenzhen, 518055, China; Luo, X., Institute of Synthetic Biology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China, Shenzhen Institute of Synthetic Biology, Shenzhen, 518055, China, CAS Key Laboratory of Quantitative Engineering Biology, Shenzhen, 518055, China; Si, T., Institute of Synthetic Biology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China, Shenzhen Institute of Synthetic Biology, Shenzhen, 518055, China, CAS Key Laboratory of Quantitative Engineering Biology, Shenzhen, 518055, China; Liu, C., Institute of Synthetic Biology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China, Shenzhen Institute of Synthetic Biology, Shenzhen, 518055, China, CAS Key Laboratory of Quantitative Engineering Biology, Shenzhen, 518055, China; Qiao, Y., Institute of Advanced Computing and Digital Engineering, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China","Traditional synthetic biology takes a trial-and-error approach, suffering from inefficiency and local optima. Recent advances in high-throughput experimental techniques generate a huge amount of biological data, which enables the use of machine learning to close the ""design-build-test-learn"" loop. Machine learning, especially deep learning, is a data-driven modeling method, which extracts useful patterns from big data and then leverages learned knowledge to tackle specific tasks. In this review, we aim to provide a brief primer of machine learning to synthetic biologists. Starting with common taxonomy, we introduce representative methods, pipelines, and underlying principles of machine learning that can be applied in synthetic biology. We include typical methods such as support vector machine, deep neural networks, generative adversarial nets, transfer learning and reinforcement learning. In particular, discriminative models, including convolutional neural networks and support vector machine, are appropriate for predicting sequence-function relationship. Generative models, including generative adversarial nets (GANs) and deep generative models for graph generation, are suitable for sequence or network design. Next, we review the recent applications of machine learning in studying synthetic biology parts and modules, including promoters, bioactive peptides, enzymes, metabolic pathways, and genetic circuits. For example, DeePromoter combined a convolutional neural network and a long-short term memory to achieve an accuracy as high as 90% when predicting promoter sequences. For enzyme design, a Gauss Process model was proposed with Bayesian optimization by upper confidence bound method, which resulted in the engineering of thermostable P450 enzymes. For antimicrobial peptides, a generative GAN model enhanced with a feedback mechanism was trained to design peptide sequences with new functions. Finally, we conclude with future challenges and directions. Particularly, interpretable machine learning models are desirable to guide mechanistic investigation. Moreover, it is necessary to develop new machine learning methods that are more compatible with biological data, which are heterogeneous, multi-modal (such as sequence, network, image, and structure), and lack of proper labels. With the increasing availability of big biological data and development of machine learning methods tailored for synthetic biology, we envision a paradigm shift towards a closed cycle of ""design-build-testlearn"" in creating artificial life with predictable functions. © 2021, Science Press. All right reserved.","Bio-networks design; Machine learning; Synthetic biology; Synthetic biology parts design","Artificial life; Availability; Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Enzymes; Model buildings; Peptides; Reinforcement learning; Support vector machines; Synthetic biology; Transfer learning; Data driven modeling methods; Experimental techniques; Function relationships; Machine learning methods; Machine learning models; Trial-and-error approach; Underlying principles; Upper confidence bound; Learning systems","Si, T.; Institute of Synthetic Biology, China; email: tong.si@siat.ac.cn
Liu, C.; Institute of Synthetic Biology, China; email: cl.liu@siat.ac.cn
Qiao, Y.; Institute of Advanced Computing and Digital Engineering, China; email: yqiao@siat.ac.cn",,"Chinese Academy of Sciences",0023074X,,,,"Chinese","Kexue Tongbao/Chin. Sc. Bull.",Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-85101332265
"Hasan Z., Roy N.","57188672564;9735486800;","Trending machine learning models in cyber-physical building environment: A survey",2021,"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",,,,"","",,,"10.1002/widm.1422","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108832636&doi=10.1002%2fwidm.1422&partnerID=40&md5=9a464a823316afd5bae2fdb0bd40d529","Information System, University of Maryland, Baltimore, MD, United States","Hasan, Z., Information System, University of Maryland, Baltimore, MD, United States; Roy, N., Information System, University of Maryland, Baltimore, MD, United States","Electricity usage of buildings (including offices, malls, and residential apartments) represents a significant portion of a nation's energy expenditure and carbon footprint. In the United States, the buildings' appliances consume 72% of the total produced electricity approximately. In this regard, cyber-physical system (CPS) researchers have put forth associated research questions to reduce cyber-physical building environment energy consumption by minimizing the energy dissipation while securing occupants' comfort. Some of the questions in CPS building include finding the optimal HVAC control, monitoring appliances' energy usage, detecting insulation problems, estimating the occupants' number and activities, managing thermal comfort, intelligently interacting with the smart grid. Various machine learning (ML) applications have been studied in recent CPS researches to improve building energy efficiency by addressing these questions. In this paper, we comprehensively review and report on the contemporary applications of ML algorithms such as deep learning, transfer learning, active learning, reinforcement learning, and other emerging techniques that propose and envision to address the above challenges in the CPS building environment. Finally, we conclude this article by discussing diverse existing open questions and prospective future directions in the CPS building environment research. This article is categorized under: Technologies > Machine Learning Technologies > Reinforcement Learning. © 2021 Wiley Periodicals LLC.","active learning; cyber-physical building environment; deep learning; reinforcement learning and control; transfer learning","Buildings; Carbon footprint; Climate control; Cyber Physical System; Deep learning; Embedded systems; Energy dissipation; Energy utilization; Engineering education; Learning systems; Reinforcement learning; Smart power grids; Thermal insulation; Transfer learning; Building energy efficiency; Building environment; Cyber-physical systems (CPS); Electricity usage; Energy expenditure; Machine learning models; Machine learning technology; Research questions; Energy efficiency","Hasan, Z.; Information System, United States; email: zhasan3@umbc.edu",,"John Wiley and Sons Inc",19424787,,,,"English","Wiley Interdiscip. Rev. Data Min. Knowl. Discov.",Review,"Article in Press","",Scopus,2-s2.0-85108832636
"Yu L., Qin S., Zhang M., Shen C., Jiang T., Guan X.","56327271300;57219790887;57221557079;57221226611;57219458347;7201463208;","A Review of Deep Reinforcement Learning for Smart Building Energy Management",2021,"IEEE Internet of Things Journal",,,,"","",,,"10.1109/JIOT.2021.3078462","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105867665&doi=10.1109%2fJIOT.2021.3078462&partnerID=40&md5=af90f12be807b99ac78e622ac355982e","College of Automation &#x0026; College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing 210003, China, and also with Xi&#x2019;an Jiaotong University, Xi&#x2019;an 710049, China. (e-mail: liang.yu@njupt.edu.cn); College of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing 210003, China.; Systems Engineering Institute, Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi&#x2019;an Jiaotong University, Xi&#x2019;an 710049, China.; School of Cyber Science and Engineering, Xi&#x2019;an Jiaotong University, Xi&#x2019;an 710049, China.; Wuhan National Laboratory for Optoelectronics, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan 430074, China.","Yu, L., College of Automation &#x0026; College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing 210003, China, and also with Xi&#x2019;an Jiaotong University, Xi&#x2019;an 710049, China. (e-mail: liang.yu@njupt.edu.cn); Qin, S., College of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing 210003, China.; Zhang, M., Systems Engineering Institute, Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi&#x2019;an Jiaotong University, Xi&#x2019;an 710049, China.; Shen, C., School of Cyber Science and Engineering, Xi&#x2019;an Jiaotong University, Xi&#x2019;an 710049, China.; Jiang, T., Wuhan National Laboratory for Optoelectronics, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan 430074, China.; Guan, X., Systems Engineering Institute, Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi&#x2019;an Jiaotong University, Xi&#x2019;an 710049, China.","Global buildings account for about 30% of the total energy consumption and carbon emission, raising severe energy and environmental concerns. Therefore, it is significant and urgent to develop novel smart building energy management (SBEM) technologies for the advance of energy-efficient and green buildings. However, it is a nontrivial task due to the following challenges. Firstly, it is generally difficult to develop an explicit building thermal dynamics model that is both accurate and efficient enough for building control. Secondly, there are many uncertain system parameters (e.g., renewable generation output, outdoor temperature, and the number of occupants). Thirdly, there are many spatially and temporally coupled operational constraints. Fourthly, building energy optimization problems can not be solved in real-time by traditional methods when they have extremely large solution spaces. Fifthly, traditional building energy management methods have respective applicable premises, which means that they have low versatility when confronted with varying building environments. With the rapid development of Internet of Things technology and computation capability, artificial intelligence technology find its significant competence in control and optimization. As a general artificial intelligence technology, deep reinforcement learning (DRL) is promising to address the above challenges. Notably, the recent years have seen the surge of DRL for SBEM. However, there lacks a systematic overview of different DRL methods for SBEM. To fill the gap, this paper provides a comprehensive review of DRL for SBEM from the perspective of system scale. In particular, we identify the existing unresolved issues and point out possible future research directions. IEEE","artificial intelligence; building microgrids.; Buildings; Deep reinforcement learning; energy management; HVAC; Internet of Things; Internet of things; Microgrids; Optimization; Reinforcement learning; Smart buildings; smart buildings; uncertainty","Deep learning; Energy management; Energy utilization; Engineering education; Intelligent buildings; Reinforcement learning; Uncertainty analysis; Artificial intelligence technologies; Building energy managements; Environmental concerns; Internet of things technologies; Operational constraints; Total energy consumption; Traditional buildings; Uncertain system parameters; Energy efficiency",,,"Institute of Electrical and Electronics Engineers Inc.",23274662,,,,"English","IEEE Internet Things J.",Article,"Article in Press","",Scopus,2-s2.0-85105867665
"Kaur J., Khan M.A., Iftikhar M., Imran M., Emad Ul Haq Q.","57221838185;57222102972;16238607000;57200275504;56786465200;","Machine Learning Techniques for 5G and beyond",2021,"IEEE Access","9",, 9321326,"23472","23488",,2,"10.1109/ACCESS.2021.3051557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099544791&doi=10.1109%2fACCESS.2021.3051557&partnerID=40&md5=7b91098a40d78dbedfc8028c5b91a50d","School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW  2678, Australia; College of Applied Computer Science, King Saud University, Riyadh, 11451, Saudi Arabia; College of Computer and Information Sciences, King Saud University, Riyadh, 11451, Saudi Arabia","Kaur, J., School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW  2678, Australia; Khan, M.A., School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW  2678, Australia; Iftikhar, M., School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW  2678, Australia; Imran, M., College of Applied Computer Science, King Saud University, Riyadh, 11451, Saudi Arabia; Emad Ul Haq, Q., College of Computer and Information Sciences, King Saud University, Riyadh, 11451, Saudi Arabia","Wireless communication systems play a very crucial role in modern society for entertainment, business, commercial, health and safety applications. These systems keep evolving from one generation to next generation and currently we are seeing deployment of fifth generation (5G) wireless systems around the world. Academics and industries are already discussing beyond 5G wireless systems which will be sixth generation (6G) of the evolution. One of the main and key components of 6G systems will be the use of Artificial Intelligence (AI) and Machine Learning (ML) for such wireless networks. Every component and building block of a wireless system that we currently are familiar with from our knowledge of wireless technologies up to 5G, such as physical, network and application layers, will involve one or another AI/ML techniques. This overview paper, presents an up-to-date review of future wireless system concepts such as 6G and role of ML techniques in these future wireless systems. In particular, we present a conceptual model for 6G and show the use and role of ML techniques in each layer of the model. We review some classical and contemporary ML techniques such as supervised and un-supervised learning, Reinforcement Learning (RL), Deep Learning (DL) and Federated Learning (FL) in the context of wireless communication systems. We conclude the paper with some future applications and research challenges in the area of ML and AI for 6G networks. © 2013 IEEE.","artificial intelligence (AI); deep learning (DL); federated learning (FL); Fifth generation (5G); machine learning (ML); reinforcement learning (RL); sixth generation (6G)","Deep learning; Entertainment industry; Learning systems; Network layers; Reinforcement learning; Future applications; Future wireless systems; Health and safety; Machine learning techniques; Research challenges; Wireless communication system; Wireless systems; Wireless technologies; 5G mobile communication systems","Kaur, J.; School of Computing and Mathematics, Australia; email: jkaur@csu.edu.au",,"Institute of Electrical and Electronics Engineers Inc.",21693536,,,,"English","IEEE Access",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85099544791
"Ma N., Aviv D., Guo H., Braham W.W.","57194323642;57195838848;57188715360;14029629800;","Measuring the right factors: A review of variables and models for thermal comfort and indoor air quality",2021,"Renewable and Sustainable Energy Reviews","135",, 110436,"","",,7,"10.1016/j.rser.2020.110436","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092170204&doi=10.1016%2fj.rser.2020.110436&partnerID=40&md5=2d53351abdb4ea5fd4675a1f6db787a2","Center for Environmental Building and Design, Weitzman School of Design, University of Pennsylvania, Philadelphia, PA  19104, United States; School of Architecture, Princeton University, Princeton, NJ  08544, United States","Ma, N., Center for Environmental Building and Design, Weitzman School of Design, University of Pennsylvania, Philadelphia, PA  19104, United States; Aviv, D., Center for Environmental Building and Design, Weitzman School of Design, University of Pennsylvania, Philadelphia, PA  19104, United States, School of Architecture, Princeton University, Princeton, NJ  08544, United States; Guo, H., School of Architecture, Princeton University, Princeton, NJ  08544, United States; Braham, W.W., Center for Environmental Building and Design, Weitzman School of Design, University of Pennsylvania, Philadelphia, PA  19104, United States","The indoor environment directly affects health and comfort as humans spend most of the day indoors. However, improperly controlled ventilation systems can expend unnecessary energy and increase health risks, while improved thermal and air quality can often result in higher energy consumption. One way to approach this dilemma is by understanding the effectiveness of the variables influencing indoor air quality (IAQ) related health and comfort. The objective of this paper is to highlight evidence and variables from empirical and deterministic models, which are combined in analytical models that current machine learning techniques often overlook. This paper reviews the analytical models and identifies the corresponding input variables, discussing their application in models based on artificial neural networks (ANNs) and reinforcement learning (RL). ANN and RL models have accurately described non-linear systems with uncertain dynamics and provided predictive and adaptive control strategies. The first part of this study focuses on the most common thermal comfort models and their variables, mainly related to steady-state and adaptive models. The second part reviews typical models of determining indoor air pollutants and their relationship with ventilation requirements and health effects. Forty-five works closely related to the field are summarized in multiple tables. The last part identifies the factors needed to predict thermal comfort and IAQ. © 2020 Elsevier Ltd","Building control; Energy efficiency; Health; Indoor air quality; Neural network; Reinforcement learning; Thermal comfort; Ventilation system","Adaptive control systems; Air quality; Analytical models; Energy utilization; Health; Health risks; Indoor air pollution; Linear systems; Quality control; Reinforcement learning; Thermal comfort; Ventilation; Adaptive control strategy; Deterministic models; Indoor air pollutants; Machine learning techniques; Thermal comfort models; Uncertain dynamics; Ventilation requirements; Ventilation systems; Learning systems","Ma, N.; Center for Environmental Building and Design, 210 S. 34th Street, United States; email: nanma1@design.upenn.edu",,"Elsevier Ltd",13640321,,RSERF,,"English","Renewable Sustainable Energy Rev",Review,"Final","All Open Access, Bronze",Scopus,2-s2.0-85092170204
"Capobianco E., Deng J.","6604078279;36820468900;","Radiomics at a glance: a few lessons learned from learning approaches",2020,"Cancers","12","9", 2453,"1","19",,,"10.3390/cancers12092453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092436260&doi=10.3390%2fcancers12092453&partnerID=40&md5=a913c6854c608039c0e40920e38cdaef","Institute for Data Science and Computing, University of Miami, Coral Gables, FL  33146, United States; Department of Therapeutic Radiology, Yale University School of Medicine, 15 York Street, New Haven, CT  06510-322, United States","Capobianco, E., Institute for Data Science and Computing, University of Miami, Coral Gables, FL  33146, United States; Deng, J., Department of Therapeutic Radiology, Yale University School of Medicine, 15 York Street, New Haven, CT  06510-322, United States","Processing and modeling medical images have traditionally represented complex tasks requiring multidisciplinary collaboration. The advent of radiomics has assigned a central role to quantitative data analytics targeting medical image features algorithmically extracted from large volumes of images. Apart from the ultimate goal of supporting diagnostic, prognostic, and therapeutic decisions, radiomics is computationally attractive due to specific strengths: scalability, efficiency, and precision. Optimization is achieved by highly sophisticated statistical and machine learning algorithms, but it is especially deep learning that stands out as the leading inference approach. Various types of hybrid learning can be considered when building complex integrative approaches aimed to deliver gains in accuracy for both classification and prediction tasks. This perspective reviews some selected learning methods by focusing on both their significance for radiomics and their unveiled potential. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Integrative inference approaches; Machine learning; Predictive modeling; Radiomics","active learning; Article; biobanking; brain tumor; breast cancer; colonoscopy; colorectal cancer; computer assisted tomography; cone beam computed tomography; data processing; deep learning; dermatology; functional magnetic resonance imaging; histopathology; human; imaging and display; lung cancer; machine learning; mammography; nuclear magnetic resonance imaging; ophthalmology; optical coherence tomography; pathology; positron emission tomography; Q learning; radiation oncology; radiogenomics; radiology; radiomics; reinforcement learning; value based reinforcement learning","Capobianco, E.; Institute for Data Science and Computing, United States; email: ecapobianco@med.miami.edu",,"MDPI AG",20726694,,,,"English","Cancers",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85092436260
"Chen L., Guo T., Liu Y.-T., Yang J.-M.","57219179374;57219180290;56021333200;57219176512;","Survey of Multi-Agent Strategy Based on Reinforcement Learning",2020,"Proceedings of the 32nd Chinese Control and Decision Conference, CCDC 2020",,, 9164559,"604","609",,1,"10.1109/CCDC49329.2020.9164559","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091560646&doi=10.1109%2fCCDC49329.2020.9164559&partnerID=40&md5=38fa965cae7731bea1a98df18601145d","Shenyang Ligong University, School of Automation and Electrical Engineering, Shenyang, 110159, China","Chen, L., Shenyang Ligong University, School of Automation and Electrical Engineering, Shenyang, 110159, China; Guo, T., Shenyang Ligong University, School of Automation and Electrical Engineering, Shenyang, 110159, China; Liu, Y.-T., Shenyang Ligong University, School of Automation and Electrical Engineering, Shenyang, 110159, China; Yang, J.-M., Shenyang Ligong University, School of Automation and Electrical Engineering, Shenyang, 110159, China","There are many multi-agent systems in life, such as driving vehicles, playing football games, and even bees building their hives. These systems are cooperative or competitive among multiple agents to fufill a task. Compared with single agent reinforcement learning, multi-agent has a larger search space, perception of other agents, and system robustness. The main purpose of this paper is to provide a clear overview of current multi-agent reinforcement learning strategy training methods, and to review the latest progress in multi-agent reinforcement learning. Finally, intorduced the application prospects and development trends of multi-agent reinforcement learning, summarized the technology of collaboration or competition. At present, multi-agent reinforcement learning has gradually been applied in many fields, such as robot systems, human-machine games, and autonomous driving. In the future, it will be widely used in resource management, transportation systems, medical care, finance and other fields. © 2020 IEEE.","Artificial Intelligence; Multi-agent; Reinforcement Learning; Strategy","Autonomous agents; Learning systems; Reinforcement learning; Social robots; Application prospect; Autonomous driving; Development trends; Multi-agent reinforcement learning; Multi-agent strategy; Resource management; System robustness; Transportation system; Multi agent systems",,,"Institute of Electrical and Electronics Engineers Inc.",,9781728158549,,,"English","Proc. Chin. Control Decis. Conf., CCDC",Conference Paper,"Final","",Scopus,2-s2.0-85091560646
"Liu C., Liu Z., Chai Y., Liu T., Chen X.","55630768600;56263344500;40760977200;55727711100;57219007428;","Virtual character behavior modeling in serious games: a review [严肃游戏中虚拟角色行为建模综述]",2020,"Journal of Image and Graphics","25","7",,"1318","1329",,,"10.11834/jig.190600","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091040418&doi=10.11834%2fjig.190600&partnerID=40&md5=e8b8b57d83d047e072f4c490aac1acf1","Faculty of Information Science and Engineering, Ningbo University, Ningbo, 315211, China; Intelligent Control Research Institute, Zhejiang Wanli University, Ningbo, 315100, China; College of Science and Technology, Ningbo University, Cixi, 315300, China","Liu, C., Faculty of Information Science and Engineering, Ningbo University, Ningbo, 315211, China, Intelligent Control Research Institute, Zhejiang Wanli University, Ningbo, 315100, China; Liu, Z., Faculty of Information Science and Engineering, Ningbo University, Ningbo, 315211, China; Chai, Y., Faculty of Information Science and Engineering, Ningbo University, Ningbo, 315211, China; Liu, T., College of Science and Technology, Ningbo University, Cixi, 315300, China; Chen, X., College of Science and Technology, Ningbo University, Cixi, 315300, China","Serious games are a new development direction of computer games. They can simulate interactive professional teaching environment and have been widely used in many fields, such as science education, rehabilitation, emergency management, and military training. Serious medical rehabilitation games are mainly used for medical technology training and rehabilitation-assisted treatment. They can train and improve the professional skills of medical staff, reduce the pain and boredom of patients during rehabilitation, and assist patients in rehabilitation treatment. Serious games in military training are mainly used in military modeling and simulation and possess controllability, security, and low cost. Virtual characters are simulated entities with life characteristics for serious games. They have credible behavior and can improve users' experience in serious games. At present, graphics rendering in serious games has gradually matured, but the research on virtual character behavior modeling is only in its first stage. A credible virtual character should have perception, emotion, and behavior. It can respond to the user's operation in time and has a certain reasoning ability. Modeling the behavior of virtual characters requires the knowledge of psychology, cognitive science, and specific domain knowledge. This paper aims to present a control algorithm that reflects the behavior of a virtual character under certain circumstances. This algorithm is an interdisciplinary application, which involves computer graphics, human-computer interaction, and artificial intelligence technologies. This paper summarized the existing studies on virtual character's behavior modeling from four aspects, namely, game plot and behavior, behavior modeling method, behavior learning, and behavior modeling evaluation. In the aspect of game plots and behavior, the design of behavior should contain the plot. The behavior of virtual characters accordingly change with the change in the plot. This condition was illustrated by the virtual character behavior design in the rehabilitation and military training plots. In the aspect of behavior modeling, the behavior modeling methods of virtual characters were summarized. The characteristics of finite state machines and behavior trees were analyzed, the behavioral learning methods were compared, the key elements of reinforcement learning were indicated, and the application of deep reinforcement learning was discussed. Reinforcement learning has four elements, namely, environment, state, action, and reward. Virtual character behavior decision learning could be completed by constantly attempting to obtain environmental rewards. In the aspect of behavior modeling evaluation, three indexes were summarized to evaluate the effectiveness of the model, namely, the real-time behavior of virtual character, the emotion-integrated behavior, and the behavior interactivity. An extremely slow character behavior response will reduce the user's interest in participating in the game, and a timely response will provide the user an efficient and pleasant experience. Users aim to achieve emotional communication with virtual characters in a virtual environment to have a good sense of immersion. A virtual character behavior framework was summarized on the basis of existing studies. It included four modules, namely, sensory input, perception analysis, behavior decision making, and action. Emotional factors are valuable to create virtual characters with credible behavior and realistic movements. They could also increase the appeal of the game. Issues that need additional research were discussed from the perspective of affective computing intervention, game story and scene design, smart phone platform, and multichannel interaction. Virtual characters should be introduced to expand the game plot and assist in teaching to completely realize the function of serious games. Serious games are different from movies. Users do not passively accept, and they learn through constant interaction. A virtual scene without any plot and any virtual character is difficult to attract users. A narrative plot can make it easy to learn. Virtual characters with behavior and emotional expressiveness can enhance users' emotional experience and guide users to learn in a real-life-like situation. The popularity of mobile terminals provides various applications for serious games and attracts many users to use serious games. Multimodel interaction would be popular in the future because single-model interaction will limit user's interaction with the virtual character. A multichannel interaction method could improve the intelligence of virtual characters and could be realized with visual, auditory, tactile, and somatosensory models. The behavior modeling of virtual characters requires comprehensive consideration of game plots, machine learning, and human-computer interaction technologies. Rigid behaviors could not attract users. Building virtual models with autonomous perception, emotion, behavior, learning ability, and multimodel interaction could immensely enhance user's immersion. These models are the development direction of serious games. Serious games provide an intuitive means for education and training. The behavior modeling of virtual characters is a developing core technology of serious games. At present, many problems need to be urgently solved. Affective computing methods, game plots, smartphone operating platforms, and multimodel human-computer interaction should be considered to improve the behavior model of virtual characters. The integration of interdisciplinary knowledge is required to create behavioral credibility, autonomous emotional expression, and convenient interactive virtual characters. These characters could provide a better learning experience for users and are easy to be promoted. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","Behavior learning; Behavior modeling; Emotion modeling; Serious game; Virtual character",,"Liu, Z.; Faculty of Information Science and Engineering, China; email: liuzhen@nbu.edu.cn",,"Editorial and Publishing Board of JIG",10068961,,,,"Chinese","J. Image and Graphics",Review,"Final","",Scopus,2-s2.0-85091040418
"Wang Z., Hong T.","56176432100;7202830430;","Reinforcement learning for building controls: The opportunities and challenges",2020,"Applied Energy","269",, 115036,"","",,29,"10.1016/j.apenergy.2020.115036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084440917&doi=10.1016%2fj.apenergy.2020.115036&partnerID=40&md5=ae2217d599b71aae82eed6aa64a03eaa","Building Technology and Urban Systems Division, Lawrence Berkeley National Laboratory, One Cyclotron Road, Berkeley, CA  94720, United States","Wang, Z., Building Technology and Urban Systems Division, Lawrence Berkeley National Laboratory, One Cyclotron Road, Berkeley, CA  94720, United States; Hong, T., Building Technology and Urban Systems Division, Lawrence Berkeley National Laboratory, One Cyclotron Road, Berkeley, CA  94720, United States","Building controls are becoming more important and complicated due to the dynamic and stochastic energy demand, on-site intermittent energy supply, as well as energy storage, making it difficult for them to be optimized by conventional control techniques. Reinforcement Learning (RL), as an emerging control technique, has attracted growing research interest and demonstrated its potential to enhance building performance while addressing some limitations of other advanced control techniques, such as model predictive control. This study conducted a comprehensive review of existing studies that applied RL for building controls. It provided a detailed breakdown of the existing RL studies that use a specific variation of each major component of the Reinforcement Learning: algorithm, state, action, reward, and environment. We found RL for building controls is still in the research stage with limited applications (11%) in real buildings. Three significant barriers prevent the adoption of RL controllers in actual building controls: (1) the training process is time consuming and data demanding, (2) the control security and robustness need to be enhanced, and (3) the generalization capabilities of RL controllers need to be improved using approaches such as transfer learning. Future research may focus on developing RL controllers that could be used in real buildings, addressing current RL challenges, such as accelerating training and enhancing control robustness, as well as developing an open-source testbed and dataset for performance benchmarking of RL controllers. © 2020 Elsevier Ltd","Building controls; Building performance; Machine learning; Optimization; Reinforcement learning","Benchmarking; Buildings; Digital storage; Model predictive control; Reinforcement learning; Robustness (control systems); Stochastic systems; Transfer learning; Building controls; Building performance; Control robustness; Control techniques; Conventional control; Generalization capability; Performance benchmarking; Research interests; Controllers; algorithm; benchmarking; building; data set; detection method; learning; training","Hong, T.; Building Technology and Urban Systems Division, One Cyclotron Road, United States; email: thong@lbl.gov",,"Elsevier Ltd",03062619,,APEND,,"English","Appl. Energy",Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85084440917
"Cui S., Tseng H.-H., Pakela J., Ten Haken R.K., El Naqa I.","57199327146;57194869441;57194145681;7006232471;57224656896;","Introduction to machine and deep learning for medical physicists",2020,"Medical Physics","47","5",,"e127","e147",,9,"10.1002/mp.14140","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084787230&doi=10.1002%2fmp.14140&partnerID=40&md5=e8dcffd12f2392e5681825c0b6842079","Department of Radiation Oncology, University of Michigan, Ann Arbor, MI  48103, United States; Applied Physics Program, University of Michigan, Ann Arbor, MI  48109, United States","Cui, S., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI  48103, United States, Applied Physics Program, University of Michigan, Ann Arbor, MI  48109, United States; Tseng, H.-H., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI  48103, United States; Pakela, J., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI  48103, United States, Applied Physics Program, University of Michigan, Ann Arbor, MI  48109, United States; Ten Haken, R.K., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI  48103, United States; El Naqa, I., Department of Radiation Oncology, University of Michigan, Ann Arbor, MI  48103, United States","Recent years have witnessed tremendous growth in the application of machine learning (ML) and deep learning (DL) techniques in medical physics. Embracing the current big data era, medical physicists equipped with these state-of-the-art tools should be able to solve pressing problems in modern radiation oncology. Here, a review of the basic aspects involved in ML/DL model building, including data processing, model training, and validation for medical physics applications is presented and discussed. Machine learning can be categorized based on the underlying task into supervised learning, unsupervised learning, or reinforcement learning; each of these categories has its own input/output dataset characteristics and aims to solve different classes of problems in medical physics ranging from automation of processes to predictive analytics. It is recognized that data size requirements may vary depending on the specific medical physics application and the nature of the algorithms applied. Data processing, which is a crucial step for model stability and precision, should be performed before training the model. Deep learning as a subset of ML is able to learn multilevel representations from raw input data, eliminating the necessity for hand crafted features in classical ML. It can be thought of as an extension of the classical linear models but with multilayer (deep) structures and nonlinear activation functions. The logic of going “deeper"" is related to learning complex data structures and its realization has been aided by recent advancements in parallel computing architectures and the development of more robust optimization methods for efficient training of these algorithms. Model validation is an essential part of ML/DL model building. Without it, the model being developed cannot be easily trusted to generalize to unseen data. Whenever applying ML/DL, one should keep in mind, according to Amara’s law, that humans may tend to overestimate the ability of a technology in the short term and underestimate its capability in the long term. To establish ML/DL role into standard clinical workflow, models considering balance between accuracy and interpretability should be developed. Machine learning/DL algorithms have potential in numerous radiation oncology applications, including automatizing mundane procedures, improving efficiency and safety of auto-contouring, treatment planning, quality assurance, motion management, and outcome predictions. Medical physicists have been at the frontiers of technology translation into medicine and they ought to be prepared to embrace the inevitable role of ML/DL in the practice of radiation oncology and lead its clinical implementation. © 2020 American Association of Physicists in Medicine","deep learning; machine learning; medical physics","Biophysics; Computation theory; Learning algorithms; Learning systems; Medical Physics; Medical problems; Model buildings; Object oriented programming; Oncology; Optimization; Parallel architectures; Predictive analytics; Quality assurance; Radiotherapy; Reinforcement learning; Automation of process; Complex data structures; Improving efficiency; Medical physics application; Nonlinear activation functions; Parallel computing architecture; Radiation oncology; Robust optimization method; Deep learning; algorithm; automation; conference paper; deep learning; human; logic; medical physicist; motion; physics; prediction; quality control; radiation oncology; reinforcement; supervised machine learning; treatment planning; unsupervised machine learning; workflow; image processing; information processing; Deep Learning; Electronic Data Processing; Image Processing, Computer-Assisted; Physics","Cui, S.; Department of Radiation Oncology, United States; email: sunan@umich.edu",,"John Wiley and Sons Ltd",00942405,,MPHYA,"32418339","English","Med. Phys.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85084787230
"Maddalena E.T., Lian Y., Jones C.N.","57193017309;57210815245;25021018100;","Data-driven methods for building control — A review and promising future directions",2020,"Control Engineering Practice","95",, 104211,"","",,13,"10.1016/j.conengprac.2019.104211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074779059&doi=10.1016%2fj.conengprac.2019.104211&partnerID=40&md5=8c2e245ffd8123c572d5d67f10744b9b","Laboratoire d'Automatique, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, 1015, Switzerland","Maddalena, E.T., Laboratoire d'Automatique, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, 1015, Switzerland; Lian, Y., Laboratoire d'Automatique, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, 1015, Switzerland; Jones, C.N., Laboratoire d'Automatique, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, 1015, Switzerland","A review of the heating, ventilation and air-conditioning control problem for buildings is presented with particular emphasis on its distinguishing features. Next, we not only examine how data-driven algorithms have been exploited to tackle the main challenges present in this area, but also point to promising future investigations both from theoretical and from practical viewpoints. Rule based control, reinforcement learning, model predictive control (MPC), and learning MPC techniques are compared on the basis of four attributes that we expect an ideal solution to possess. Finally, on-line learning MPC with guarantees is recognized as an approach with high potential that needs to be further investigated by researchers. Such a solution is likely to be accepted by practitioners since it meets the industry expectations of reduced deployment time and costs. © 2019 Elsevier Ltd","Building control; Heating ventilation and air-conditioning (HVAC); Machine learning; Model predictive control (MPC); Reinforcement learning","HVAC; Learning systems; Machine learning; Model predictive control; Predictive control systems; Reinforcement learning; Building controls; Data-driven algorithm; Data-driven methods; Deployment time; Heating ventilation and air conditioning; Ideal solutions; Rule-based control; Ventilation and air conditioning; Air conditioning","Maddalena, E.T.; Laboratoire d'Automatique, Switzerland; email: emilio.maddalena@epfl.ch",,"Elsevier Ltd",09670661,,COEPE,,"English","Control Eng. Pract.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85074779059
"Yan J., Yang S., Hancock E.","36026971200;57211496553;7202876234;","Learning for graph matching and related combinatorial optimization problems",2020,"IJCAI International Joint Conference on Artificial Intelligence","2021-January",,,"4988","4996",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097343858&partnerID=40&md5=e6105f9a31213e8090c2b4bf52f1d431","Department of CSE, MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University, China; Ant Financial Services Group; Department of Computer Science, University of York, United Kingdom","Yan, J., Department of CSE, MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University, China; Yang, S., Ant Financial Services Group; Hancock, E., Department of Computer Science, University of York, United Kingdom","This survey gives a selective review of recent development of machine learning (ML) for combinatorial optimization (CO), especially for graph matching. The synergy of these two well-developed areas (ML and CO) can potentially give transformative change to artificial intelligence, whose foundation relates to these two building blocks. For its representativeness and wide-applicability, this paper is more focused on the problem of weighted graph matching, especially from the learning perspective. For graph matching, we show that many learning techniques e.g. convolutional neural networks, graph neural networks, reinforcement learning can be effectively incorporated in the paradigm for extracting the node features, graph structure features, and even the matching engine. We further present outlook for the new settings for learning graph matching, and direction towards more integrated combinatorial optimization solvers with prediction models, and also the mutual embrace of traditional solver and machine learning components. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.",,"Combinatorial optimization; Convolutional neural networks; Graph structures; Graphic methods; Predictive analytics; Reinforcement learning; Building blockes; Combinatorial optimization problems; Graph neural networks; Learning graphs; Learning techniques; Matching engines; Prediction model; Structure features; Learning systems","Yan, J.; Department of CSE, China; email: yanjunchi@sjtu.edu.cn","Bessiere C.","International Joint Conferences on Artificial Intelligence",10450823,9780999241165,,,"English","IJCAI Int. Joint Conf. Artif. Intell.",Conference Paper,"Final","",Scopus,2-s2.0-85097343858
[No author name available],[No author id available],"26th International Conference on Information and Software Technologies, ICIST 2020",2020,"Communications in Computer and Information Science","1283 CCIS",,,"","",382,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093097040&partnerID=40&md5=09339bb25fa95a81ba812645b0784a58",,"","The proceedings contain 30 papers. The special focus in this conference is on Information and Software Technologies. The topics include: Genetic Optimization Approach to Construct Schedule for Service Staff; sigma Key Agreement Protocol for e-Banking System; table Header Correction Algorithm Based on Heuristics for Improving Spreadsheet Data Extraction; a Review of Self-balancing Robot Reinforcement Learning Algorithms; exploring Web Service QoS Estimation for Web Service Composition; random Forests and Homogeneous Granulation; orchestration Security Challenges in the Fog Computing; a Novel Edge Detection Operator for Identifying Buildings in Augmented Reality Applications; military Vehicle Recognition with Different Image Machine Learning Techniques; a Novel Model Driven Framework for Image Enhancement and Object Recognition; run-Time Class Generation: Algorithm for Decomposition of Homogeneous Classes; twitter Based Classification for Personal and Non-personal Heart Disease Claims; escape the Lab: Chemical Experiments in Virtual Reality; The VOIL Digital Transformation Competence Framework. Evaluation and Design of Higher Education Curricula; gamified Evaluation in Game-Based Learning; hyperparameter Tuning Using Automated Methods to Improve Models for Predicting Student Success; a Case Study of Applying Gamification in Teaching Project Management; design of the Platform Solutions to Increase the Employability and E-Learning Opportunities for Low Skilled Women; cross-lingual Metaphor Paraphrase Detection – Experimental Corpus and Baselines; deep Learning-Based Part-of-Speech Tagging of the Tigrinya Language; Knowledge-Based Generation of the UML Dynamic Models from the Enterprise Model Illustrated by the Ticket Buying Process Example; tag Me If You Can: Insights into the Challenges of Supporting Unrestricted P2P News Tagging.",,,,"Lopata A.Butkiene R.Gudoniene D.Sukacke V.","Springer Science and Business Media Deutschland GmbH",18650929,9783030595050,,,"English","Commun. Comput. Info. Sci.",Conference Review,"Final","",Scopus,2-s2.0-85093097040
[No author name available],[No author id available],"8th World Conference on Information Systems and Technologies, WorldCIST 2020",2020,"Advances in Intelligent Systems and Computing","1160 AISC",,,"","",1408,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086272176&partnerID=40&md5=963ca4f548c0573d1d9533491b1435b3",,"","The proceedings contain 135 papers. The special focus in this conference is on Information Systems and Technologies. The topics include: Social care services for older adults: Paper registration versus a web-based platform registration; enabling green building’s comfort using information and communication technologies: A systematic review of the literature; Exploring the innovative aspects of CV distributed ledgers based on blockchain; eFish – An innovative fresh fish evaluation system; a virtual reality approach to automatic blood sample generation; Building information modeling academic assessment: international architecture workshop UPC-UAM; radar system for the reconstruction of 3D Objects: A preliminary study; Achieving stronger compaction for DCT-Based Steganography: A region-growing approach; teaching computer programming as well-defined domain for beginners with protoboard; machine learning and data networks: Perspectives, feasibility, and opportunities; underground channel model for visible light wireless communication based on neural networks; mpCUBIC: A CUBIC-like congestion control algorithm for multipath TCP; context-Aware mobile applications in fog infrastructure: A literature review; characterizing the cost of introducing secure programming patterns and practices in ethereum; analyzing IoT-Based botnet malware activity with distributed low interaction honeypots; Evolution of HTTPS usage by Portuguese municipalities; augmented reality to enhance visitors’ experience at archaeological sites; Design and performance analysis for intelligent F-PMIPv6 mobility support for smart manufacturing; development of trustworthy self-adaptive framework for wireless sensor networks; traffic Flow Prediction Using Public Transport and Weather Data: A Medium Sized City Case Study; filtering users accounts for enhancing the results of social media mining tasks; from reinforcement learning towards artificial general intelligence; preface.",,,,"Rocha A.Adeli H.Reis L.P.Costanzo S.Orovic I.Moreira F.","Springer",21945357,9783030456900,,,"English","Adv. Intell. Sys. Comput.",Conference Review,"Final","",Scopus,2-s2.0-85086272176
"AboElHamd E., Shamma H.M., Saleh M.","57211156452;35079258500;55216860300;","Dynamic programming models for maximizing customer lifetime value: An overview",2020,"Advances in Intelligent Systems and Computing","1037",,,"419","445",,1,"10.1007/978-3-030-29516-5_34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072845842&doi=10.1007%2f978-3-030-29516-5_34&partnerID=40&md5=55c693c3e71e5d98ce8993d0206216aa","Department of Operations Research and Decision Support, Cairo University, Giza, Egypt; Department of Management, School of Business, The American University in Cairo, New Cairo, Egypt","AboElHamd, E., Department of Operations Research and Decision Support, Cairo University, Giza, Egypt; Shamma, H.M., Department of Management, School of Business, The American University in Cairo, New Cairo, Egypt; Saleh, M., Department of Operations Research and Decision Support, Cairo University, Giza, Egypt","Customer lifetime value (CLV) is the most reliable indicator in direct marketing for measuring the profitability of the customers. This motivated the researchers to compete in building models to maximize CLV and consequently, enhancing the firm, and the customer relationship. This review paper analyzes the contributions of applying dynamic programming models in the area of direct marketing, to maximize CLV. It starts by reviewing the basic models that focused on calculating CLV, measuring it, simulating, optimizing it or -rarely- maximizing its value. Then highlighting the dynamic programming models including, Markov Decision Process (MDP), Approximate Dynamic Programming (ADP), also called Reinforcement Learning (RL), Deep RL and Double Deep RL. Although, MDP contributed significantly in the area of maximizing CLV, it has many limitations that encouraged researchers to utilize ADP (i.e. RL) and recently deep reinforcement learning (i.e. deep Q network). These algorithms overcame the limitations of MDP and were able to solve complex problems without suffering from the curse of dimensionality problem, however they still have some limitations including, overestimating the action values. This was the main motivation behind proposing double deep Q networks (DDQN). Meanwhile, neither DDQN nor the algorithms that outperformed it and overcame its limitations were applied in the area of direct marketing and this leaves a space for future research directions. © Springer Nature Switzerland AG 2020.","Approximate Dynamic Programming (ADP); Customer Lifetime Value (CLV); Deep Q Network (DQN); Deep Reinforcement Learning (DRL); Markov Decision Process (MDP); Reinforcement Learning (RL)","Commerce; Deep learning; Intelligent systems; Machine learning; Markov processes; Public relations; Reinforcement learning; Sales; Value engineering; Approximate dynamic programming; Customer lifetime value; Deep Q Network (DQN); Deep Reinforcement Learning (DRL); Markov Decision Processes; Dynamic programming","AboElHamd, E.; Department of Operations Research and Decision Support, Egypt; email: e.aboelhamd1@gmail.com","Bi Y.Bhatia R.Kapoor S.","Springer Verlag",21945357,9783030295158,,,"English","Adv. Intell. Sys. Comput.",Conference Paper,"Final","",Scopus,2-s2.0-85072845842
"Garr E.","57188558392;","Contributions of the basal ganglia to action sequence learning and performance",2019,"Neuroscience and Biobehavioral Reviews","107",,,"279","295",,5,"10.1016/j.neubiorev.2019.09.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072558944&doi=10.1016%2fj.neubiorev.2019.09.017&partnerID=40&md5=897a17e891a7a2bd8ad2053a6738cf2b","Graduate Center, City University of New York, 365 5th Avenue, New York, NY  10016, United States","Garr, E., Graduate Center, City University of New York, 365 5th Avenue, New York, NY  10016, United States","Animals engage in intricately woven and choreographed action sequences that are constructed from trial-and-error learning. The mechanisms by which the brain links together individual actions which are later recalled as fluid chains of behavior are not fully understood, but there is broad consensus that the basal ganglia play a crucial role in this process. This paper presents a comprehensive review of the role of the basal ganglia in action sequencing, with a focus on whether the computational framework of reinforcement learning can capture key behavioral features of sequencing and the neural mechanisms that underlie them. While a simple neurocomputational model of reinforcement learning can capture key features of action sequence learning, this model is not sufficient to capture goal-directed control of sequences or their hierarchical representation. The hierarchical structure of action sequences, in particular, poses a challenge for building better models of action sequencing, and it is in this regard that further investigations into basal ganglia information processing may be informative. © 2019 Elsevier Ltd","Action sequence; Basal ganglia; Dopamine; Execution; Initiation; Motor cortex; Reinforcement learning; striatum; Termination","basal ganglion; brain function; cell function; computer model; corpus striatum; decision making; human; instrumental conditioning; motor cortex; movement (physiology); neuroscience; nonhuman; prediction; priority journal; reinforcement; Review; sequence learning; stimulus response; animal; biological model; brain; cognition; learning; nerve tract; physiology; Animals; Brain; Cognition; Humans; Learning; Models, Neurological; Neural Pathways; Reinforcement, Psychology",,,"Elsevier Ltd",01497634,,NBRED,"31541637","English","Neurosci. Biobehav. Rev.",Review,"Final","All Open Access, Green",Scopus,2-s2.0-85072558944
"Siddhu M.K., Yaakob S.N.","57209262792;55394447500;","Deep learning applied to arabic and latin scripts: A review",2019,"International Journal of Scientific and Technology Research","8","11",,"1510","1521",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075045437&partnerID=40&md5=f2ac68081c1fa1b5095d46864010a1d7","Universiti Malaysia Perlis, Malaysia; School of computer and communications engineering, Universiti Malaysia Perlis, Malaysia","Siddhu, M.K., Universiti Malaysia Perlis, Malaysia; Yaakob, S.N., School of computer and communications engineering, Universiti Malaysia Perlis, Malaysia","Over the last few years deep learning has out classed traditional machine learning in several domains like machine translation, computer vision, speech recognition, natural language processing etc. The advent of neural networks based architectures and deep reinf orcement learning has revolutionized the field of machine learning. Document analysis community has also taken advantage of this new era. Recentl y deep learning methods employed in document analysis have enjoyed tremendous success. These methods are robust to deformations, scaling and rotation. Therefore, they are best suited for text recognition. Particularly, convolutional neural networks and recurrent neural networks coupled with embedded attributes have been exploited extensively in word spotting as well as text recognition. In this paper, we summarize and compare most significant deep learning techniques used so far in Arabic, Urdu, Pashto and Latin scripts. A brief introduction to the state-of-the-art frameworks and libraries for building deep learning based systems is introduced at the end of the article. The article concludes by identifying unexplored possibilities which may serve as guidelines for future work. © IJSTR 2019.","Deep reinforcement Learning; Embedded attributes; Handwriting recognition; Index Terms: Convolutional Neural Networks; Recurrent neural networks; Siamese Networks; Word spotting",,,,"International Journal of Scientific and Technology Research",22778616,,,,"English","Int. J. Sci. Technol. Res.",Article,"Final","",Scopus,2-s2.0-85075045437
"Han M., May R., Zhang X., Wang X., Pan S., Yan D., Jin Y., Xu L.","55345227200;57210581265;57209625490;57221484870;55839694200;7401863008;57199284864;56142333100;","A review of reinforcement learning methodologies for controlling occupant comfort in buildings",2019,"Sustainable Cities and Society","51",, 101748,"","",,23,"10.1016/j.scs.2019.101748","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070980900&doi=10.1016%2fj.scs.2019.101748&partnerID=40&md5=5e7e7a5d6833ff65558c534475c064b8","School of Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Beijing Key Laboratory of Green Built Environment and Energy Efficient Technology, Beijing University of Technology, Beijing, 100124, China; Building Energy Conservation Research Center, Tsinghua University, Beijing, 100084, China; School of Management, Xi'an Jiaotong University, No.28 Xianning West Road, Xi'an, China","Han, M., School of Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; May, R., School of Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Zhang, X., School of Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Wang, X., School of Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Pan, S., Beijing Key Laboratory of Green Built Environment and Energy Efficient Technology, Beijing University of Technology, Beijing, 100124, China; Yan, D., Building Energy Conservation Research Center, Tsinghua University, Beijing, 100084, China; Jin, Y., Building Energy Conservation Research Center, Tsinghua University, Beijing, 100084, China; Xu, L., School of Management, Xi'an Jiaotong University, No.28 Xianning West Road, Xi'an, China","Classical building control systems are becoming vulnerable with increasing complexities in contemporary built environments and energy systems. Due to this, the reinforcement learning (RL) method is becoming more distinctive and applicable in control networks for buildings. This paper, therefore, conducts a comprehensive review of RL techniques applied in control systems for occupant comfort in indoor built environments. The empirical applications of RL-based control systems are presented, depending on comfort objectives (thermal comfort, indoor air quality, and lighting) along with other objectives which invariably includes energy consumption. The class of RL algorithms and implementation details regarding how the value functions have been represented and how the policies are improved are also illustrated. This paper shows there are limited works for which RL has been explored for controlling occupant comfort, especially in indoor air quality and lighting. Relatively few of the reviewed works incorporate occupancy patterns and/or occupant feedback into the control loop. Moreover, this paper identifies a gap with regard to the performance of implementing cooperative multi-agent RL (MARL). Based on our findings, current challenges and further opportunities are discussed. We expect to clarify the feasible theory and functions of RL for building control systems, which would promote their wider-spread application in built environments. © 2019 Elsevier Ltd","Building; Control; Indoor comfort; Occupant; Reinforcement learning","Air quality; Buildings; Control engineering; Control theory; Energy utilization; Fertilizers; Indoor air pollution; Lighting; Machine learning; Multi agent systems; Reinforcement learning; Building control systems; Built environment; Indoor air quality; Indoor comforts; Occupant; Occupant comforts; Occupant feedbacks; Reinforcement learning method; Quality control","Zhang, X.; School of Technology and Business Studies, Sweden; email: xza@du.se",,"Elsevier Ltd",22106707,,,,"English","Sustainable Cities Soc.",Review,"Final","",Scopus,2-s2.0-85070980900
"May R., Zhang X., Wu J., Han M.","57210581265;57209625490;55927434900;55345227200;","Reinforcement learning control for indoor comfort: A survey",2019,"IOP Conference Series: Materials Science and Engineering","609","6", 062011,"","",,,"10.1088/1757-899X/609/6/062011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074523665&doi=10.1088%2f1757-899X%2f609%2f6%2f062011&partnerID=40&md5=7c00ab087c6472821cfc7bf6f1dcde48","School of Industrial Technology and Business Studies, Dalarna University, Falun, 79188, Sweden","May, R., School of Industrial Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Zhang, X., School of Industrial Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Wu, J., School of Industrial Technology and Business Studies, Dalarna University, Falun, 79188, Sweden; Han, M., School of Industrial Technology and Business Studies, Dalarna University, Falun, 79188, Sweden","Building control systems are prone to fail in complex and dynamic environments. The reinforcement learning (RL) method is becoming more and more attractive in automatic control. The success of the reinforcement learning method in many artificial intelligence applications has resulted in an open question on how to implement the method in building control systems. This paper therefore conducts a comprehensive review of the RL methods applied in control systems for indoor comfort and environment. The empirical applications of RL-based control systems are then presented, depending on optimisation objectives and the measurement of energy use. This paper illustrates the class of algorithms and implementation details regarding how the value functions have been represented and how the policies are improved. This paper is expected to clarify the feasible theory and functions of RL for building control systems, which would promote their wider-spread application and thus contribute to the social economic benefits in the energy and built environments. © 2019 IOP Publishing Ltd. All rights reserved.",,"Air quality; Automation; Buildings; Control theory; Energy conservation; Historic preservation; Indoor air pollution; Machine learning; Reinforcement learning; Building control systems; Built environment; Dynamic environments; Indoor comforts; Reinforcement learning control; Reinforcement learning method; Social-economic; Value functions; Zero energy buildings","Han, M.; School of Industrial Technology and Business Studies, Sweden; email: mea@du.se","Berardi U.Allard F.","Institute of Physics Publishing",17578981,,,,"English","IOP Conf. Ser. Mater. Sci. Eng.",Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85074523665
"Jara-Ettinger J.","56117206100;","Theory of mind as inverse reinforcement learning",2019,"Current Opinion in Behavioral Sciences","29",,,"105","110",,13,"10.1016/j.cobeha.2019.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066985252&doi=10.1016%2fj.cobeha.2019.04.010&partnerID=40&md5=9f1d5df8d8cf8e70a84f1392624777e0","Department of Psychology, Yale University, New Haven, CT, United States; Department of Computer Science, Yale University, New Haven, CT, United States","Jara-Ettinger, J., Department of Psychology, Yale University, New Haven, CT, United States, Department of Computer Science, Yale University, New Haven, CT, United States","We review the idea that Theory of Mind—our ability to reason about other people's mental states—can be formalized as inverse reinforcement learning. Under this framework, expectations about how mental states produce behavior are captured in a reinforcement learning (RL) model. Predicting other people's actions is achieved by simulating a RL model with the hypothesized beliefs and desires, while mental-state inference is achieved by inverting this model. Although many advances in inverse reinforcement learning (IRL) did not have human Theory of Mind in mind, here we focus on what they reveal when conceptualized as cognitive theories. We discuss landmark successes of IRL, and key challenges in building human-like Theory of Mind. © 2019 Elsevier Ltd",,"human; human experiment; reinforcement; review; theory of mind",,,"Elsevier Ltd",23521546,,,,"English","Curr. Opin. Behav. Sci.",Review,"Final","",Scopus,2-s2.0-85066985252
"Hamrick J.B.","55922007400;","Analogues of mental simulation and imagination in deep learning",2019,"Current Opinion in Behavioral Sciences","29",,,"8","16",,13,"10.1016/j.cobeha.2018.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059463875&doi=10.1016%2fj.cobeha.2018.12.011&partnerID=40&md5=b022e3cc0b1d70a7a41bee9103064097","DeepMind, 6 Pancras Square, London, N1C 4AG, United Kingdom","Hamrick, J.B., DeepMind, 6 Pancras Square, London, N1C 4AG, United Kingdom","Mental simulation — the capacity to imagine what will or what could be — is a salient feature of human cognition, playing a key role in a wide range of cognitive abilities. In artificial intelligence, the last few years have seen the development of methods which are analogous to mental models and mental simulation. This paper outlines recent methods in deep learning for constructing such models from data and learning to use them via reinforcement learning, and compares such approaches to human mental simulation. Model-based methods in deep learning can serve as powerful tools for building and scaling cognitive models. However, a number of challenges remain in matching the capacity of human mental simulation for efficiency, compositionality, generalization, and creativity. © 2019",,"controlled study; creativity; human; human experiment; imagination; reinforcement; review; simulation",,,"Elsevier Ltd",23521546,,,,"English","Curr. Opin. Behav. Sci.",Review,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85059463875
"Mason K., Grijalva S.","56954387300;6602833144;","A review of reinforcement learning for autonomous building energy management",2019,"Computers and Electrical Engineering","78",,,"300","312",,28,"10.1016/j.compeleceng.2019.07.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073644795&doi=10.1016%2fj.compeleceng.2019.07.019&partnerID=40&md5=a4c4e700fe69f057a3aa001f57c2a1bc","School of Electrical and Computer Engineering, Georgia Institute of Technology, AtlantaGA, United States","Mason, K., School of Electrical and Computer Engineering, Georgia Institute of Technology, AtlantaGA, United States; Grijalva, S., School of Electrical and Computer Engineering, Georgia Institute of Technology, AtlantaGA, United States","The area of building energy management has received a significant amount of interest in recent years. This area is concerned with combining advancements in sensor technologies, communications and advanced control algorithms to optimize energy utilization. Reinforcement learning is one of the most prominent machine learning algorithms used for control problems and has had many successful applications in the area of building energy management. This research gives a comprehensive review of the literature relating to the application of reinforcement learning to developing autonomous building energy management systems. Energy savings of greater than 20% are reported in the literature for more complex building energy management problems when implementing reinforcement learning. The main direction for future research and challenges in reinforcement learning are also outlined. © 2019 Elsevier Ltd","Building energy management; Deep learning; Machine learning; Reinforcement learning; Smart grid; Smart homes","Automation; Deep learning; Energy conservation; Energy management; Energy management systems; Energy utilization; Intelligent buildings; Learning algorithms; Learning systems; Reinforcement learning; Smart power grids; Advanced control algorithms; Building energy management systems; Building energy managements; Complex buildings; Control problems; Sensor technologies; Smart grid; Smart homes; Machine learning","Mason, K.; School of Electrical and Computer Engineering, Atlanta, United States; email: kmason35@gatech.edu",,"Elsevier Ltd",00457906,,CPEEB,,"English","Comput Electr Eng",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85073644795
"Vázquez-Canteli J.R., Nagy Z.","57195835393;24829852300;","Reinforcement learning for demand response: A review of algorithms and modeling techniques",2019,"Applied Energy","235",,,"1072","1089",,141,"10.1016/j.apenergy.2018.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056655519&doi=10.1016%2fj.apenergy.2018.11.002&partnerID=40&md5=8d3e6e80d9ffce98f57fd5f365f104f3","Intelligent Environments Laboratory, Department of Civil, Architectural and Environental Engineering, The University of Texas at Austin, Austin, TX  78712, United States","Vázquez-Canteli, J.R., Intelligent Environments Laboratory, Department of Civil, Architectural and Environental Engineering, The University of Texas at Austin, Austin, TX  78712, United States; Nagy, Z., Intelligent Environments Laboratory, Department of Civil, Architectural and Environental Engineering, The University of Texas at Austin, Austin, TX  78712, United States","Buildings account for about 40% of the global energy consumption. Renewable energy resources are one possibility to mitigate the dependence of residential buildings on the electrical grid. However, their integration into the existing grid infrastructure must be done carefully to avoid instability, and guarantee availability and security of supply. Demand response, or demand-side management, improves grid stability by increasing demand flexibility, and shifts peak demand towards periods of peak renewable energy generation by providing consumers with economic incentives. This paper reviews the use of reinforcement learning, a machine learning algorithm, for demand response applications in the smart grid. Reinforcement learning has been utilized to control diverse energy systems such as electric vehicles, heating ventilation and air conditioning (HVAC) systems, smart appliances, or batteries. The future of demand response greatly depends on its ability to prevent consumer discomfort and integrate human feedback into the control loop. Reinforcement learning is a potentially model-free algorithm that can adapt to its environment, as well as to human preferences by directly integrating user feedback into its control logic. Our review shows that, although many papers consider human comfort and satisfaction, most of them focus on single-agent systems with demand-independent electricity prices and a stationary environment. However, when electricity prices are modelled as demand-dependent variables, there is a risk of shifting the peak demand rather than shaving it. We identify a need to further explore reinforcement learning to coordinate multi-agent systems that can participate in demand response programs under demand-dependent electricity prices. Finally, we discuss directions for future research, e.g., quantifying how RL could adapt to changing urban conditions such as building refurbishment and urban or population growth. © 2018 Elsevier Ltd","Building energy; Deep learning; Electric vehicles; HVAC control; Machine learning; Smart grid","Air conditioning; Costs; Deep learning; Electric automobiles; Electric power transmission networks; Electric utilities; Electric vehicles; Energy utilization; Feedback; Learning systems; Multi agent systems; Population statistics; Reinforcement learning; Renewable energy resources; Smart power grids; Urban growth; Building energy; Building refurbishments; Demand response programs; Heating ventilation and air conditioning; HVAC control; Renewable energy generation; Smart grid; Stationary environments; Learning algorithms; air conditioning; algorithm; building; demand analysis; electric vehicle; heating; machine learning; modeling; power generation; refurbishment; smart grid; ventilation","Nagy, Z.; Intelligent Environments Laboratory, United States; email: nagy@utexas.edu",,"Elsevier Ltd",03062619,,APEND,,"English","Appl. Energy",Review,"Final","",Scopus,2-s2.0-85056655519
"Unkelbach C., Koch A., Alves H.","12787875700;49561437500;49561093000;","The evaluative information ecology: On the frequency and diversity of “good” and “bad”",2019,"European Review of Social Psychology","30","1",,"216","270",,8,"10.1080/10463283.2019.1688474","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075526763&doi=10.1080%2f10463283.2019.1688474&partnerID=40&md5=fc99be380afd23910fbe192dad8474af","Social Cognition Center Cologne, Universität zu Köln, Köln, Germany; Booth School of Business, University of Chicago, Chicago, IL, United States","Unkelbach, C., Social Cognition Center Cologne, Universität zu Köln, Köln, Germany; Koch, A., Booth School of Business, University of Chicago, Chicago, IL, United States; Alves, H., Social Cognition Center Cologne, Universität zu Köln, Köln, Germany","We propose the Evaluative Information Ecology (EvIE) model as a model of the social environment. It makes two assumptions: Positive “good” information is more frequent compared to negative “bad” information and positive information is more similar and less diverse compared to negative information. We review support for these two properties based on psycho-lexical studies (e.g., negative trait words are used less frequently but they are more diverse), studies on affective reactions (e.g., people experience positive emotions more frequently but negative emotions are more diverse), and studies using direct similarity assessments (i.e., people rate positive information as more similar/less diverse compared to negative information). Next, we suggest explanations for the two properties building on potential adaptive advantages, reinforcement learning, hedonistic sampling processes, similarity from co-occurrence, and similarity from restricted ranges. Finally, we provide examples of how the EvIE model refines well-established effects (e.g., intergroup biases; preferences for groups without motivation or intent) and how it leads to the discovery of novel phenomena (e.g., the common good phenomenon; people share positive traits but negative traits make them distinct). We close by discussing the benefits relative to the drawbacks of ecological approaches in social psychology and how an ecological and cognitive level of analysis may complement each other. © 2019, © 2019 European Association of Social Psychology.","ecology; Evaluation; halo effects; intergroup biases; person perception",,"Unkelbach, C.; Social Cognition Center Cologne, Richard-Strauss-Str. 2, Germany; email: christian.unkelbach@uni-koeln.de",,"Taylor and Francis Inc.",10463283,,,,"English","Eur. Rev. Soc. Psychol.",Article,"Final","",Scopus,2-s2.0-85075526763
"Mammeri Z.","6603913199;","Reinforcement learning based routing in networks: Review and classification of approaches",2019,"IEEE Access","7",, 8701570,"55916","55950",,29,"10.1109/ACCESS.2019.2913776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066846240&doi=10.1109%2fACCESS.2019.2913776&partnerID=40&md5=ed8bee72232086c0d8ea639969cecc2e","IRIT, Paul Sabatier University, Toulouse, 31062, France","Mammeri, Z., IRIT, Paul Sabatier University, Toulouse, 31062, France","Reinforcement learning (RL), which is a class of machine learning, provides a framework by which a system can learn from its previous interactions with its environment to efficiently select its actions in the future. RL has been used in a number of application fields, including game playing, robotics and control, networks, and telecommunications, for building autonomous systems that improve themselves with experience. It is commonly accepted that RL is suitable for solving optimization problems related to distributed systems in general and to routing in networks in particular. RL also has reasonable overhead-in terms of control packets, memory and computation-compared to other optimization techniques used to solve the same problems. Since the mid-1990s, over 60 protocols have been proposed, with major or minor contributions in the field of optimal route selection to convey packets in different types of communication networks under various user QoS requirements. This paper provides a comprehensive review of the literature on the topic. The review is structured in a way that shows how network characteristics and requirements were gradually considered over time. Classification criteria are proposed to present and qualitatively compare existing RL-based routing protocols. © 2019 IEEE.","communication networks; path optimization; quality of service; Reinforcement learning; routing protocols","Internet protocols; Machine learning; Network routing; Quality of service; Routing protocols; Telecommunication networks; Transportation routes; Application fields; Autonomous systems; Classification criterion; Distributed systems; Network characteristics; Optimization problems; Optimization techniques; Path optimizations; Reinforcement learning","Mammeri, Z.; IRIT, France; email: zoubir.mammeri@irit.fr",,"Institute of Electrical and Electronics Engineers Inc.",21693536,,,,"English","IEEE Access",Review,"Final","All Open Access, Gold",Scopus,2-s2.0-85066846240
"Stanley K.O., Clune J., Lehman J., Miikkulainen R.","7102875151;23388416900;36439865800;7003434895;","Designing neural networks through neuroevolution",2019,"Nature Machine Intelligence","1","1",,"24","35",,131,"10.1038/s42256-018-0006-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064166007&doi=10.1038%2fs42256-018-0006-z&partnerID=40&md5=c1eac8ccf3921746a22422ce0e9c233e","Uber AI Labs, San Francisco, CA, United States; University of Central Florida, Orlando, FL, United States; University of Wyoming, Laramie, WY, United States; Sentient Technologies, San Francisco, CA, United States; The University of Texas at Austin, Austin, TX, United States","Stanley, K.O., Uber AI Labs, San Francisco, CA, United States, University of Central Florida, Orlando, FL, United States; Clune, J., Uber AI Labs, San Francisco, CA, United States, University of Wyoming, Laramie, WY, United States; Lehman, J., Uber AI Labs, San Francisco, CA, United States; Miikkulainen, R., Sentient Technologies, San Francisco, CA, United States, The University of Texas at Austin, Austin, TX, United States","Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field’s contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence. © 2019, Springer Nature Limited.",,"Evolutionary algorithms; Gradient methods; Learning algorithms; Machine learning; Network architecture; Reinforcement learning; Stochastic systems; Activation functions; Artificial general intelligences; Evolutionary process; Gradient-based neural networks; Large-scale computing; Learning neural networks; Machine learning communities; Stochastic gradient descent; Deep learning","Stanley, K.O.; Uber AI LabsUnited States; email: kstanley@uber.com",,"Nature Research",25225839,,,,"English","Nat. Mach. Intell.",Review,"Final","",Scopus,2-s2.0-85064166007
"Zhavoronkov A., Mamoshina P., Vanhaelen Q., Scheibye-Knudsen M., Moskalev A., Aliper A.","39862415800;56893719500;54790233000;25633162000;7003730453;54889030500;","Artificial intelligence for aging and longevity research: Recent advances and perspectives",2019,"Ageing Research Reviews","49",,,"49","66",,32,"10.1016/j.arr.2018.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057217558&doi=10.1016%2fj.arr.2018.11.003&partnerID=40&md5=152533229cefe42429ddfe38415868eb","Pharmaceutical Artificial Intelligence Department, Insilico Medicine, Inc., Baltimore, MD, United States; Biogerontology Research Foundation, London, United Kingdom; Buck Institute for Research on Aging, Novato, CA, United States; Department of Computer Science, University of Oxford, Oxford, United Kingdom; Center for Healthy Aging, Department of Cellular and Molecular Medicine, University of Copenhagen, Denmark; George Mason University, Fairfax, VA, United States","Zhavoronkov, A., Pharmaceutical Artificial Intelligence Department, Insilico Medicine, Inc., Baltimore, MD, United States, Biogerontology Research Foundation, London, United Kingdom, Buck Institute for Research on Aging, Novato, CA, United States; Mamoshina, P., Pharmaceutical Artificial Intelligence Department, Insilico Medicine, Inc., Baltimore, MD, United States, Department of Computer Science, University of Oxford, Oxford, United Kingdom; Vanhaelen, Q., Pharmaceutical Artificial Intelligence Department, Insilico Medicine, Inc., Baltimore, MD, United States; Scheibye-Knudsen, M., Center for Healthy Aging, Department of Cellular and Molecular Medicine, University of Copenhagen, Denmark; Moskalev, A., George Mason University, Fairfax, VA, United States; Aliper, A., Pharmaceutical Artificial Intelligence Department, Insilico Medicine, Inc., Baltimore, MD, United States","The applications of modern artificial intelligence (AI) algorithms within the field of aging research offer tremendous opportunities. Aging is an almost universal unifying feature possessed by all living organisms, tissues, and cells. Modern deep learning techniques used to develop age predictors offer new possibilities for formerly incompatible dynamic and static data types. AI biomarkers of aging enable a holistic view of biological processes and allow for novel methods for building causal models—extracting the most important features and identifying biological targets and mechanisms. Recent developments in generative adversarial networks (GANs) and reinforcement learning (RL) permit the generation of diverse synthetic molecular and patient data, identification of novel biological targets, and generation of novel molecular compounds with desired properties and geroprotectors. These novel techniques can be combined into a unified, seamless end-to-end biomarker development, target identification, drug discovery and real world evidence pipeline that may help accelerate and improve pharmaceutical research and development practices. Modern AI is therefore expected to contribute to the credibility and prominence of longevity biotechnology in the healthcare and pharmaceutical industry, and to the convergence of countless areas of research. © 2018","Aging biomarker; Artificial intelligence; Deep learning; Drug discovery; Generative adversarial networks; Metalearning; Reinforcement learning; Symbolic learning","aging; artificial intelligence; data base; deep learning; drug industry; epigenetics; gene therapy; generative adversarial network; health care; human; immunosenescence; longevity; machine learning; nuclear magnetic resonance imaging; patient coding; patient identification; personalized medicine; regenerative medicine; reinforcement; reinforcement learning; Review; transfer of learning; algorithm; animal; drug development; factual database; medical research; biological marker; Algorithms; Animals; Artificial Intelligence; Biomarkers; Biomedical Research; Databases, Factual; Drug Discovery; Humans; Longevity","Vanhaelen, Q.; Pharmaceutical Artificial Intelligence Department, United States; email: vanhaelen@insilicomedicine.com",,"Elsevier Ireland Ltd",15681637,,ARRGA,"30472217","English","Ageing Res. Rev.",Review,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85057217558
"Behrens T.E.J., Muller T.H., Whittington J.C.R., Mark S., Baram A.B., Stachenfeld K.L., Kurth-Nelson Z.","8069852700;56389123200;57193897928;36463923700;57204182197;55532749300;21833994400;","What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior",2018,"Neuron","100","2",,"490","509",,135,"10.1016/j.neuron.2018.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054781498&doi=10.1016%2fj.neuron.2018.10.002&partnerID=40&md5=14236ade63eaab4ff64bebefc5368044","Wellcome Centre for Integrative Neuroimaging, Centre for Functional Magnetic Resonance Imaging of the Brain, University of Oxford, John Radcliffe Hospital, Oxford, OX3 9DU, United Kingdom; Wellcome Centre for Human Neuroimaging, Institute of Neurology, University College London, 12 Queen Square, London, WC1N 3BG, United Kingdom; DeepMind, London, United Kingdom; Max Planck-UCL Centre for Computational Psychiatry and Ageing Research, University College London, London, United Kingdom","Behrens, T.E.J., Wellcome Centre for Integrative Neuroimaging, Centre for Functional Magnetic Resonance Imaging of the Brain, University of Oxford, John Radcliffe Hospital, Oxford, OX3 9DU, United Kingdom, Wellcome Centre for Human Neuroimaging, Institute of Neurology, University College London, 12 Queen Square, London, WC1N 3BG, United Kingdom; Muller, T.H., Wellcome Centre for Integrative Neuroimaging, Centre for Functional Magnetic Resonance Imaging of the Brain, University of Oxford, John Radcliffe Hospital, Oxford, OX3 9DU, United Kingdom; Whittington, J.C.R., Wellcome Centre for Integrative Neuroimaging, Centre for Functional Magnetic Resonance Imaging of the Brain, University of Oxford, John Radcliffe Hospital, Oxford, OX3 9DU, United Kingdom; Mark, S., Wellcome Centre for Human Neuroimaging, Institute of Neurology, University College London, 12 Queen Square, London, WC1N 3BG, United Kingdom; Baram, A.B., Wellcome Centre for Integrative Neuroimaging, Centre for Functional Magnetic Resonance Imaging of the Brain, University of Oxford, John Radcliffe Hospital, Oxford, OX3 9DU, United Kingdom; Stachenfeld, K.L., DeepMind, London, United Kingdom; Kurth-Nelson, Z., DeepMind, London, United Kingdom, Max Planck-UCL Centre for Computational Psychiatry and Ageing Research, University College London, London, United Kingdom","It is proposed that a cognitive map encoding the relationships between entities in the world supports flexible behavior, but the majority of the neural evidence for such a system comes from studies of spatial navigation. Recent work describing neuronal parallels between spatial and non-spatial behaviors has rekindled the notion of a systematic organization of knowledge across multiple domains. We review experimental evidence and theoretical frameworks that point to principles unifying these apparently disparate functions. These principles describe how to learn and use abstract, generalizable knowledge and suggest that map-like representations observed in a spatial context may be an instance of general coding mechanisms capable of organizing knowledge of all kinds. We highlight how artificial agents endowed with such principles exhibit flexible behavior and learn map-like representations observed in the brain. Finally, we speculate on how these principles may offer insight into the extreme generalizations, abstractions, and inferences that characterize human cognition. Behrens et al. review an emerging field building formalisms for understanding the neural basis of flexible behavior. The authors extend these ideas toward representations useful for generalization and structural abstraction, allowing rapid inferences and flexible behavior with little direct experience. © 2018 Elsevier Inc.","Cognitive Map; Decision Making; Generalization; Hippocampal Formation; Inference; Prefrontal Cortex; Reinforcement Learning; Spatial Cognition; Statistical Learning; Structure Learning","article; decision making; hippocampus; human; human experiment; prefrontal cortex; reinforcement; biological model; brain; mental function; physiology; Brain; Humans; Mental Processes; Models, Neurological","Behrens, T.E.J.; Wellcome Centre for Integrative Neuroimaging, United Kingdom; email: behrens@fmrib.ox.ac.uk",,"Cell Press",08966273,,NERNE,"30359611","English","Neuron",Review,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85054781498
"Doncieux S., Filliat D., Díaz-Rodríguez N., Hospedales T., Duro R., Coninx A., Roijers D.M., Girard B., Perrin N., Sigaud O.","14833739000;55976331500;55904010200;23990799300;7003592275;54891754100;55939408300;57189212413;35786632400;6508371727;","Open-ended learning: A conceptual framework based on representational redescription",2018,"Frontiers in Neurorobotics","12","SEP", 59,"","",,16,"10.3389/fnbot.2018.00059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055086379&doi=10.3389%2ffnbot.2018.00059&partnerID=40&md5=fbf963bdeb7154d40b171ea1b153768e","Sorbonne Université, CNRS, ISIR, Paris, France; U2IS, INRIA Flowers, ENSTA ParisTech, Palaiseau, France; Institute of Perception, Action and Behaviour, University of Edinburgh, Edinburgh, United Kingdom; GII, Universidade da Coruña, A Coruña, Spain; Department of Computer Science, Vrije Universiteit Amsterdam, Amsterdam, Netherlands","Doncieux, S., Sorbonne Université, CNRS, ISIR, Paris, France; Filliat, D., U2IS, INRIA Flowers, ENSTA ParisTech, Palaiseau, France; Díaz-Rodríguez, N., U2IS, INRIA Flowers, ENSTA ParisTech, Palaiseau, France; Hospedales, T., Institute of Perception, Action and Behaviour, University of Edinburgh, Edinburgh, United Kingdom; Duro, R., GII, Universidade da Coruña, A Coruña, Spain; Coninx, A., Sorbonne Université, CNRS, ISIR, Paris, France; Roijers, D.M., Department of Computer Science, Vrije Universiteit Amsterdam, Amsterdam, Netherlands; Girard, B., Sorbonne Université, CNRS, ISIR, Paris, France; Perrin, N., Sorbonne Université, CNRS, ISIR, Paris, France; Sigaud, O., Sorbonne Université, CNRS, ISIR, Paris, France","Reinforcement learning (RL) aims at building a policy that maximizes a task-related reward within a given domain. When the domain is known, i.e., when its states, actions and reward are defined, Markov Decision Processes (MDPs) provide a convenient theoretical framework to formalize RL. But in an open-ended learning process, an agent or robot must solve an unbounded sequence of tasks that are not known in advance and the corresponding MDPs cannot be built at design time. This defines the main challenges of open-ended learning: how can the agent learn how to behave appropriately when the adequate states, actions and rewards representations are not given? In this paper, we propose a conceptual framework to address this question. We assume an agent endowed with low-level perception and action capabilities. This agent receives an external reward when it faces a task. It must discover the state and action representations that will let it cast the tasks as MDPs in order to solve them by RL. The relevance of the action or state representation is critical for the agent to learn efficiently. Considering that the agent starts with a low level, task-agnostic state and action spaces based on its low-level perception and action capabilities, we describe open-ended learning as the challenge of building the adequate representation of states and actions, i.e., of redescribing available representations. We suggest an iterative approach to this problem based on several successive Representational Redescription processes, and highlight the corresponding challenges in which intrinsic motivations play a key role. © 2007 - 2018 Frontiers Media S.A. All Rights Reserved.","Actions and goals; Developmental robotics; Reinforcement learning; Representational redescription; Skills; State representation learning","Machine design; Markov processes; Actions and goals; Developmental robotics; Representational redescription; Skills; State representation; Reinforcement learning; Agnostic; conceptual framework; decision making; human; motivation; perception; reinforcement; review; reward; robotics; skill","Doncieux, S.; Sorbonne Université, France; email: stephane.doncieux@sorbonne-universite.fr",,"Frontiers Media S.A.",16625218,,,,"English","Front. Neurorobotics",Review,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85055086379
"Aghakhani H., MacHiry A., Nilizadeh S., Kruegel C., Vigna G.","57203548141;55848928300;54387329400;14017971800;7003452317;","Detecting deceptive reviews using generative adversarial networks",2018,"Proceedings - 2018 IEEE Symposium on Security and Privacy Workshops, SPW 2018",,, 8424638,"89","95",,15,"10.1109/SPW.2018.00022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052206535&doi=10.1109%2fSPW.2018.00022&partnerID=40&md5=893b424c6f9f26b5194389d6bce9e29d","University of California, Santa Barbara, United States; Carnegie Mellon University Silicon Valley, United States","Aghakhani, H., University of California, Santa Barbara, United States; MacHiry, A., University of California, Santa Barbara, United States; Nilizadeh, S., Carnegie Mellon University Silicon Valley, United States; Kruegel, C., University of California, Santa Barbara, United States; Vigna, G., University of California, Santa Barbara, United States","In the past few years, consumer review sites have become the main target of deceptive opinion spam, where fictitious opinions or reviews are deliberately written to sound authentic. Most of the existing work to detect the deceptive reviews focus on building supervised classifiers based on syntactic and lexical patterns of an opinion. With the successful use of Neural Networks on various classification applications, in this paper, we propose FakeGAN a system that for the first time augments and adopts Generative Adversarial Networks (GANs) for a text classification task, in particular, detecting deceptive reviews. Unlike standard GAN models which have a single Generator and Discriminator model, FakeGAN uses two discriminator models and one generative model. The generator is modeled as a stochastic policy agent in reinforcement learning (RL), and the discriminators use Monte Carlo search algorithm to estimate and pass the intermediate action-value as the RL reward to the generator. Providing the generator model with two discriminator models avoids the mod collapse issue by learning from both distributions of truthful and deceptive reviews. Indeed, our experiments show that using two discriminators provides FakeGAN high stability, which is a known issue for GAN architectures. While FakeGAN is built upon a semi-supervised classifier, known for less accuracy, our evaluation results on a dataset of TripAdvisor hotel reviews show the same performance in terms of accuracy as of the state-of-the-art approaches that apply supervised machine learning. These results indicate that GANs can be effective for text classification tasks. Specifically, FakeGAN is effective at detecting deceptive reviews. © 2018 IEEE.","Deceptive opinion spam; Generative adversarial networks","Reinforcement learning; Stochastic systems; Supervised learning; Text processing; Adversarial networks; Deceptive opinion spam; Evaluation results; Generator modeling; State-of-the-art approach; Supervised classifiers; Supervised machine learning; Text classification; Classification (of information)",,,"Institute of Electrical and Electronics Engineers Inc.",,9780769563497,,,"English","Proc. - IEEE Symp. Secur. Priv. Workshops, SPW",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85052206535
"Mackevicius E.L., Fee M.S.","56531975800;7003419527;","Building a state space for song learning",2018,"Current Opinion in Neurobiology","49",,,"59","68",,13,"10.1016/j.conb.2017.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038248530&doi=10.1016%2fj.conb.2017.12.001&partnerID=40&md5=c60252702dda3615cba6df4da724f616","Department of Brain and Cognitive Sciences, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA  46-5133, United States","Mackevicius, E.L., Department of Brain and Cognitive Sciences, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA  46-5133, United States; Fee, M.S., Department of Brain and Cognitive Sciences, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA  46-5133, United States","The songbird system has shed light on how the brain produces precisely timed behavioral sequences, and how the brain implements reinforcement learning (RL). RL is a powerful strategy for learning what action to produce in each state, but requires a unique representation of the states involved in the task. Songbird RL circuitry is thought to operate using a representation of each moment within song syllables, consistent with the sparse sequential bursting of neurons in premotor cortical nucleus HVC. However, such sparse sequences are not present in very young birds, which sing highly variable syllables of random lengths. Here, we review and expand upon a model for how the songbird brain could construct latent sequences to support RL, in light of new data elucidating connections between HVC and auditory cortical areas. We hypothesize that learning occurs via four distinct plasticity processes: 1) formation of ‘tutor memory’ sequences in auditory areas; 2) formation of appropriately-timed latent HVC sequences, seeded by inputs from auditory areas spontaneously replaying the tutor song; 3) strengthening, during spontaneous replay, of connections from HVC to auditory neurons of corresponding timing in the ‘tutor memory’ sequence, aligning auditory and motor representations for subsequent song evaluation; and 4) strengthening of connections from premotor neurons to motor output neurons that produce the desired sounds, via well-described song RL circuitry. © 2017",,"auditory cortex; brain nerve cell; learning; memory; motor system; music; nerve cell plasticity; nonhuman; premotor cortex; priority journal; reinforcement; Review; songbird; animal; biological model; brain; cytology; learning; nerve cell; nerve tract; physiology; time factor; vocalization; Animals; Brain; Learning; Models, Neurological; Neural Pathways; Neurons; Reinforcement (Psychology); Songbirds; Time Factors; Vocalization, Animal",,,"Elsevier Ltd",09594388,,COPUE,"29268193","English","Curr. Opin. Neurobiol.",Review,"Final","All Open Access, Green",Scopus,2-s2.0-85038248530
[No author name available],[No author id available],"8th SESAR Innovation Days, SIDs 2018",2018,"SESAR Innovation Days",,,,"","",273,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078980481&partnerID=40&md5=0ec43bf1fbed87219edadcba8355c3d7",,"","The proceedings contain 34 papers. The special focus in this conference is on SESAR Innovation Days. The topics include: Open flight trajectories for reproducible ANS performance review; assessment of the future air traffic management system safety performance using network-based simulation model; building blocks of assistant based speech recognition for air traffic management applications; visual analytics of flight trajectories for uncovering decision making strategies; detecting controllers’ actions in past mode S data by autoencoder-based anomaly detection; flight level prediction with a deep feedforward network; occupancy peak estimation from sector geometry and traffic flow data; arrival trade-offs considering total flight and passenger delays and fairness; Application of machine learning for ATM performance assessment – Identification of sources of en-route flight inefficiency; aircraft atypical approach detection using functional principal component analysis; identification of complexity factors for remote towers; drone information service requirements for U-space; The effects of the introduction of free route (HUFRA, hungarian free route airspace) in the hungarian airspace; towards new metrics assessing air traffic network interactions; airline disruption management with aircraft swapping and reinforcement learning; SESAR 1 solutions implementation: Key feature-high performing airport operations; introducing competition through auctions in the air traffic control market; a boosted tree framework for runway occupancy and exit prediction; A multi-layer model for long-term KPI alignment forecasts; optimal aircraft path planning in a structured airspace using ensemble weather forecasts; smart data fusion: Probabilistic record linkage adapted to merge two trajectories from different sources; decision support for an optimal choice of subsidised routes in air transportation.",,,,,"SESAR Joint Undertaking",07701268,,,,"English","SESAR Innov. Days, SIDs",Conference Review,"Final","",Scopus,2-s2.0-85078980481
"Nagy Z., Park J.Y., Vazquez-Canteli J.","24829852300;57200016003;57195835393;","Reinforcement learning for smart buildings and cities",2017,"Proceedings of 33rd PLEA International Conference: Design to Thrive, PLEA 2017","2",,,"2618","2624",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085912844&partnerID=40&md5=3419490f1b59e958180a440cae6d344b","Intelligent Environments Laboratory, Department of Civil, Architectural and Environmental Engineering, University of Texas at Austin, Austin, TX, United States","Nagy, Z., Intelligent Environments Laboratory, Department of Civil, Architectural and Environmental Engineering, University of Texas at Austin, Austin, TX, United States; Park, J.Y., Intelligent Environments Laboratory, Department of Civil, Architectural and Environmental Engineering, University of Texas at Austin, Austin, TX, United States; Vazquez-Canteli, J., Intelligent Environments Laboratory, Department of Civil, Architectural and Environmental Engineering, University of Texas at Austin, Austin, TX, United States","The built environment is responsible for about 30% of the anthropogenic greenhouse gas emissions, and, thus, offering a large reduction potential. Advances in information and communication technologies made widespread monitoring of environmental conditions and building operation possible, and have sparked the notion of smart buildings. The promise of smart buildings is that these large amounts of data can be used in advanced control algorithms to ensure optimal, energy efficient behavior. In this paper, we discuss a specific machine learning method called reinforcement learning (RL). RL is an agent based control method, in which the agent/controller performs actions in the environment for which it receives a reward. We argue that RL is particularly interesting for applications in the built environment. First, RL is a model-less approach, suited for large, complex models. Second, RL has been successful in applications where sequential decision making is important, e.g., in building and urban scale energy systems. Finally, due to its interactive nature, RL is the sole machine learning method capable of learning directly from humans, allowing the control system to adapt over time to occupants. We review key developments of RL in the built environment, and discuss the challenges and opportunities. Copyright © NCEUB 2017.","Artificial intelligence; Human - building interaction; Reinforcement learning; Smart buildings; Smart cities","Buildings; Decision making; Energy efficiency; Environmental technology; Gas emissions; Greenhouse gases; Reinforcement learning; Structural design; Advanced control algorithms; Anthropogenic greenhouse gas emissions; Environmental conditions; Information and Communication Technologies; Large amounts of data; Machine learning methods; Reduction potential; Sequential decision making; Learning systems","Nagy, Z.; Intelligent Environments Laboratory, United States; email: nagy@utexas.edu","Brotas L.Roaf S.Nicol F.","NCEUB 2017 - Network for Comfort and Energy Use in Buildings",,9780992895754,,,"English","Proc. PLEA Int. Conf.: Des. Thrive, PLEA",Conference Paper,"Final","",Scopus,2-s2.0-85085912844
"Rogers N., Killcross S., Curnoe D.","56973564600;6701613447;6602541175;","Hunting for evidence of cognitive planning: Archaeological signatures versus psychological realities",2016,"Journal of Archaeological Science: Reports","5",,,"225","239",,2,"10.1016/j.jasrep.2015.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947966715&doi=10.1016%2fj.jasrep.2015.11.002&partnerID=40&md5=2c557a95772efc67cf7a948d7c382525","School of Biological, Earth and Environmental Sciences, Faculty of Science, UNSW Australia, Sydney, NSW  2052, Australia; School of Psychology, Faculty of Science, UNSW Australia, Sydney, NSW  2052, Australia","Rogers, N., School of Biological, Earth and Environmental Sciences, Faculty of Science, UNSW Australia, Sydney, NSW  2052, Australia; Killcross, S., School of Psychology, Faculty of Science, UNSW Australia, Sydney, NSW  2052, Australia; Curnoe, D., School of Biological, Earth and Environmental Sciences, Faculty of Science, UNSW Australia, Sydney, NSW  2052, Australia","Cognitive planning is acknowledged as one of the hallmarks of modern cognition. However, identifying objective evidence of cognitive planning in the archaeological record has been difficult and controversial. While some archaeologists have argued that so-called behaviourally ""archaic"" Homo sapiens and Homo neanderthalensis were unable to plan, others have proposed that complex material culture could not have been produced without sophisticated planning abilities. There is agreement, however, that evidence for cognitive planning can readily be found in the archaeological record. This review presents an alternative interpretation based on research in psychology, neuropsychology and reinforcement learning. It outlines alternative mechanisms that can drive behaviour including goal-directed actions, habits, hierarchical reinforcement learning and fixed action patterns. We contest current archaeological theory by arguing that: 1) for methodological reasons, evidence for cognitive planning cannot be found in the archaeological record and, 2) basic learning processes, based on contingency and contiguity, are powerful enough to be the building blocks of substantially more complex behaviours including the acquisition and ""invention"" of technological behaviours. We suggest that cognitive archaeology focus on collaborative projects to empirically test existing theories utilising techniques such as neuroimaging, dual-task paradigms and mathematical modelling. Such experiments would greatly improve the concordance of archaeological theories with those of allied disciplines. © 2015.","Archaeology; Cognition; Goal-directed behaviour; Knapping; Planning; Stimulus-response learning",,"Rogers, N.; School of Biological, Australia; email: n.rogers@unsw.edu.au",,"Elsevier Ltd",2352409X,,,,"English","J.Archaeol. Sci. Rep.",Review,"Final","",Scopus,2-s2.0-84947966715
[No author name available],[No author id available],"15th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2016",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9852 LNAI",,,"1","824",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988614808&partnerID=40&md5=7e767d35b052178c7d0e43ec918b1a73",,"","The proceedings contain 50 papers. The special focus in this conference is on Machine Learning and Knowledge Discovery in Databases. The topics include: AUC-Maximized deep convolutional neural fields for protein sequence labeling; a novel incremental covariance-guided one-class support vector machine; learning efficiently in semantic based regularization; persistent roles in online social networks; a split-merge DP-means algorithm to avoid local minima; efficient distributed decision trees for robust regression; fast hoeffding drift detection method for evolving data streams; differentially private user data perturbation with multi-level privacy controls; on dynamic feature weighting for feature drifting data streams; modeling sequential preferences with dynamic user and context factors; node re-ordering as a means of anomaly detection in time-evolving graphs; building ensembles of adaptive nested dichotomies with random-pair selection; credible review detection with limited information using consistency features; interactive visual data exploration with subjective feedback; coupled hierarchical dirichlet process mixtures for simultaneous clustering and topic modeling; enhancing traffic congestion estimation with social media by coupled hidden Markov model; learning distributed representations of users for source detection in online social networks; linear bandits in unknown environments; estimating labels from rough group comparisons; detecting public influence on news using topic-aware dynamic granger test; exploring a mixed representation for encoding temporal coherence; a tree-based subgoal discovery method to accelerate reinforcement learning; selecting collaborative filtering algorithms using metalearning and measuring the stability of feature selection.",,,,"Landwehr N.Giuseppe J.Manco G.Frasconi P.","Springer Verlag",03029743,9783319462264,,,"English","Lect. Notes Comput. Sci.",Conference Review,"Final","",Scopus,2-s2.0-84988614808
"Khamassi M., Humphries M.D.","6508136456;7101607621;","Integrating cortico-limbic-basal ganglia architectures for learning model-based and model-free navigation strategies",2012,"Frontiers in Behavioral Neuroscience",,"OCTOBER 2012",,"","",,43,"10.3389/fnbeh.2012.00079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869006148&doi=10.3389%2ffnbeh.2012.00079&partnerID=40&md5=2b445f1e8bc25c8dceb5dfe675201923","Institut des Systèmes Intelligents et de Robotique, Université Pierre et Marie Curie, 75005 Paris, France; UMR7222, Centre National de la Recherche Scientifique, 75005 Paris, France; Group for Neural Theory, Department d'Etudes Cognitives, 29 rue d'Ulm, 75005 Paris, France; Faculty of Life Sciences, University of Manchester, Manchester M60 1QD, United Kingdom","Khamassi, M., Institut des Systèmes Intelligents et de Robotique, Université Pierre et Marie Curie, 75005 Paris, France, UMR7222, Centre National de la Recherche Scientifique, 75005 Paris, France; Humphries, M.D., Group for Neural Theory, Department d'Etudes Cognitives, 29 rue d'Ulm, 75005 Paris, France, Faculty of Life Sciences, University of Manchester, Manchester M60 1QD, United Kingdom","Behaviour in spatial navigation is often organised into map-based (placedriven) versus map-free (cue-driven) strategies; behaviour in operant conditioning research is often organised into goal-directed versus habitual strategies. Here we attempt to unify the two. We review one powerful theory for distinct forms of learning during instrumental conditioning, namely model-based (maintaining a representation of the world) and model-free (reacting to immediate stimuli) learning algorithms. We extend these lines of argument to propose an alternative taxonomy for spatial navigation, showing how various previously identified strategies can be distinguished as ""model-based"" or ""model-free"" depending on the usage of information and not on the type of information (e.g. cue vs place). We argue that identifying ""modelfree"" learning with dorsolateral striatum and ""model-based"" learning with dorsomedial striatum could reconcile numerous conflicting results in the spatial navigation literature. From this perspective, we further propose that the ventral striatum plays key roles in the model-building process. We propose that the core of the ventral striatum is positioned to learn the probability of action selection for every transition between states of the world. We further review suggestions that the ventral striatal core and shell are positioned to act as ""critics"" contributing to the computation of a reward prediction error for model-free and model-based systems, respectively. © 2012 Khamassi and Humphries.","Action-outcome; Basal ganglia; Nucleus accumbens; Reinforcement learning; Stimulus-response","alpha amino 3 hydroxy 5 methyl 4 isoxazolepropionic acid; n methyl dextro aspartic acid; association; basal ganglion; BOLD signal; corpus striatum; decision making; functional magnetic resonance imaging; habit; habituation; hippocampus; instrumental conditioning; learning; learning algorithm; limbic cortex; locomotion; motivation; neuronavigation; nucleus accumbens; reinforcement; review; sensitivity analysis; stimulus response; task performance; taxonomy","Khamassi, M.; UPMC, 4 place Jussieu, 75005 Paris, France; email: mehdi.khamassi@isir.upmc.fr",,,16625153,,,,"English","Front. Behav. Neurosci.",Review,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-84869006148
"Botvinick M.M., Niv Y., Barto A.G.","6602977616;22958379700;7003417191;","Hierarchically organised behaviour and its neural foundations: A reinforcement-learning perspective",2011,"Modelling Natural Action Selection",,,,"264","299",,1,"10.1017/CBO9780511731525.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923559585&doi=10.1017%2fCBO9780511731525.017&partnerID=40&md5=dd12e4a73b9c91865cf0797a6dd56c1a","Princeton Neuroscience Institute and Department of Psychology, Princeton UniversityNJ, United States; Department of Computer Science, University of Massachusetts, Amherst, MA, United States","Botvinick, M.M., Princeton Neuroscience Institute and Department of Psychology, Princeton UniversityNJ, United States; Niv, Y., Princeton Neuroscience Institute and Department of Psychology, Princeton UniversityNJ, United States; Barto, A.G., Department of Computer Science, University of Massachusetts, Amherst, MA, United States","Research on human and animal behaviour has long emphasised its hierarchical structure – the divisibility of ongoing behaviour into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behaviour has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this chapter, we re-examine behavioural hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behaviour. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behaviour. In recent years, it has become increasingly common within both psychology and neuroscience to explore the applicability of ideas from machine learning. Indeed, one can now cite numerous instances where this strategy has been fruitful. Arguably, however, no area of machine learning has had as profound and sustained an impact on psychology and neuroscience as that of computational reinforcement learning (RL). The impact of RL was initially felt in research on classical and instrumental conditioning (Barto and Sutton, 1981; Sutton and Barto, 1990; Wickens et al., 1995). Soon thereafter, its impact extended to research on midbrain dopaminergic function, where the temporal-difference learning paradigm provided a framework for interpreting temporal profiles of dopaminergic activity (Barto, 1995; Houk et al., 1995; Montague et al., 1996; Schultz et al., 1997). Subsequently, actor–critic architectures for RL have inspired new interpretations of functional divisions of labour within the basal ganglia and cerebral cortex (see Joel et al., 2002, for a review), and RL-based accounts have been advanced to address issues as diverse as motor control (e.g., Miyamoto et al., 2004), working memory (e.g., O’Reilly and Frank, 2006), performance monitoring (e.g., Holroyd and Coles, 2002), and the distinction between habitual and goal-directed behaviour (e.g., Daw et al., 2005). © Cambridge University Press 2012.",,,,,"Cambridge University Press",,9780511731525; 9781107000490,,,"English","Modelling Natural Action Selection",Book Chapter,"Final","",Scopus,2-s2.0-84923559585
"Werbos P.J.","7003827430;","Intelligence in the brain: A theory of how it works and how to build it",2009,"Neural Networks","22","3",,"200","212",,139,"10.1016/j.neunet.2009.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349247013&doi=10.1016%2fj.neunet.2009.03.012&partnerID=40&md5=9b3232ab875fafb3c50413677595b2e5","ECCS Division, National Science Foundation (NSF), Room 525, Arlington, VA 22230, United States","Werbos, P.J., ECCS Division, National Science Foundation (NSF), Room 525, Arlington, VA 22230, United States","This paper presents a theory of how general-purpose learning-based intelligence is achieved in the mammal brain, and how we can replicate it. It reviews four generations of ever more powerful general-purpose learning designs in Adaptive, Approximate Dynamic Programming (ADP), which includes reinforcement learning as a special case. It reviews empirical results which fit the theory, and suggests important new directions for research, within the scope of NSF's recent initiative on Cognitive Optimization and Prediction. The appendices suggest possible connections to the realms of human subjective experience, comparative cognitive neuroscience, and new challenges in electric power. The major challenge before us today in mathematical neural networks is to replicate the ""mouse level"", but the paper does contain a few thoughts about building, understanding and nourishing levels of general intelligence beyond the mouse.","Adaptive critic; Adaptive dynamic programming; ADP; Approximate dynamic programming; Backpropagation; Cognitive prediction; Comparative neuropsychology; Complexity; Consciousness; Creativity; Freud; Intelligence; Neurocontrol; Reinforcement learning; Robust","Adaptive critic; Adaptive dynamic programming; ADP; Approximate dynamic programming; Cognitive prediction; Comparative neuropsychology; Complexity; Consciousness; Creativity; Freud; Intelligence; Neurocontrol; Robust; Backpropagation; Education; Neural networks; Psychophysiology; Reinforcement; Reinforcement learning; Systems engineering; Dynamic programming; article; artificial intelligence; behavior; brain; cognition; creativity; empirical research; engineering; experience; human; imagination; intelligence; learning; mathematical model; nerve cell; nerve cell network; neuroscience; nonhuman; prediction; priority journal; theory; Animals; Artificial Intelligence; Brain; Cognition; Humans; Intelligence; Learning; Mind-Body Relations (Metaphysics); Neural Networks (Computer); Reinforcement (Psychology); Software","Werbos, P.J.; ECCS Division, Room 525, Arlington, VA 22230, United States; email: werbos@ieee.org",,,08936080,,NNETE,"19386468","English","Neural Netw.",Article,"Final","",Scopus,2-s2.0-67349247013
"Thiery C., Scherrer B.","35099538100;55903675600;","Building controllers for tetris",2009,"ICGA Journal","32","1",,"3","11",,31,"10.3233/ICG-2009-32102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350140182&doi=10.3233%2fICG-2009-32102&partnerID=40&md5=da78ad760848981083e08b6de499ee8b","LORIA - INRIA Lorraine, Campus Scientifique, BP 239, 54506 Vandoeuvre-lés-Nancy Cedex, France","Thiery, C., LORIA - INRIA Lorraine, Campus Scientifique, BP 239, 54506 Vandoeuvre-lés-Nancy Cedex, France; Scherrer, B.","This article has two purposes: a review on the problem of building a controller for the well-known video game Tetris, and a contribution on how to achieve the best performance. Key components of typical solutions include feature design and feature-weight optimization. We provide a list of all the features we could find in the literature and in implementations, and mention the methods that have been used for weight optimization. We also highlight the fact that performance measures for Tetris must be compared with great care, as (1) they have a rather large variance, and (2) subtle implementation choices can have a significant effect on the resulting scores. An immediate interest of this review is illustrated. Straightforwardly gathering ideas from different works may lead to new ideas. We show how we built a controller that outperforms the previously known best controllers. Finally, we briefly discuss how this implementation allowed us to win the Tetris-domain prize of the 2008 Reinforcement Learning Competition.",,"Reinforcement learning; Feature weight; Performance measure; Video game; Weight optimization; Controllers","Thiery, C.; LORIA - INRIA Lorraine, BP 239, 54506 Vandoeuvre-lés-Nancy Cedex, France; email: thierych@loria.fr",,"Tilburg Centre for Cogination and Communication",13896911,,,,"English","ICGA J.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-70350140182
"Werbos P.J.","7003827430;","Using ADP to understand and replicate brain intelligence: The next level design",2007,"Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning, ADPRL 2007",,, 4220835,"209","216",,63,"10.1109/ADPRL.2007.368190","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548766755&doi=10.1109%2fADPRL.2007.368190&partnerID=40&md5=4b599fe12e3fcb918727255cf5b5117e","National Science Foundation, Arlington, VA 22203, United States","Werbos, P.J., National Science Foundation, Arlington, VA 22203, United States","Since the 1960's I proposed that we could understand and replicate the highest level of intelligence seen in the brain, by building ever more capable and general systems for adaptive dynamic programming (ADP) - like ""reinforcement learning"" but based on approximating the Bellman equation and allowing the controller to know its utility function. Growing empirical evidence on the brain supports this approach. Adaptive critic systems now meet tough engineering challenges and provide a kind of first-generation model of the brain. Lewis, Prokhorov and myself have early second-generation work. Mammal brains possess three core capabilities - creativity/imagination and ways to manage spatial and temporal complexity - even beyond the second generation. This paper reviews previous progress, and describes new tools and approaches to overcome the spatial complexity gap. © 2007 IEEE.",,"Adaptive systems; Computational complexity; Mathematical models; Reinforcement learning; Adaptive critic systems; Bellman equations; Dynamic programming","Werbos, P.J.; National Science Foundation, Arlington, VA 22203, United States; email: pwerbos@nsf.gov",,,,1424407060; 9781424407064,,,"English","Proc. IEEE Symp. Approx. Dyn. Program. Reinf. Learn.",Conference Paper,"Final","",Scopus,2-s2.0-34548766755
"Werbos P.J.","7003827430;","Using ADP to understand and replicate brain intelligence: The next level design?",2007,"Understanding Complex Systems","2007",,,"109","123",,10,"10.1007/978-3-540-73267-9_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548260004&doi=10.1007%2f978-3-540-73267-9_6&partnerID=40&md5=f51830e97dee65b9aeccae4f63fcb14c",,"Werbos, P.J.","Since the 196's I proposed that we could understand and replicate the highest level of intelligence seen in the brain, by building ever more capable and general systems for adaptive dynamic programming (ADP) - like ""reinforcement learning"" but based on approximating the Bellman equation and allowing the controller to know its utility function. Growing empirical evidence on the brain supports this approach. Adaptive critic systems now meet tough engineering challenges and provide a kind of first-generation model of the brain. Lewis, Prokhorov and I have done some work on second-generation designs. I now argue that mammal brains possess three core capabilities - creativity/ imagination and ways to manage spatial and temporal complexity - even beyond the second generation. This chapter reviews previous progress, and describes new tools and approaches to overcome the spatial complexity gap. The Appendices discuss what we can learn about higher functions of the human mind from this kind of mathematical approaches. © 2007 Springer-Verlag Berlin Heidelberg.",,,,"Perlovsky L.I.Kozma R.",,18600832,3540732667; 9783540732662,,,"English","Underst. Complex Syst.",Review,"Final","",Scopus,2-s2.0-34548260004
"Fernández F., Veloso M.","7401589140;7006520632;","Reusing and building a policy library",2006,"ICAPS 2006 - Proceedings, Sixteenth International Conference on Automated Planning and Scheduling","2006",,,"378","381",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746095398&partnerID=40&md5=23d6945f0b29f6960c691eb0e45e9b83","Computer Science Department, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, United States","Fernández, F., Computer Science Department, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, United States; Veloso, M., Computer Science Department, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, United States","Policy Reuse is a method to improve reinforcement learning with the ability to solve multiple tasks by building upon past problem solving experience, as accumulated in a Policy Library. Given a new task, a Policy Reuse learner uses the past policies in the library as a probabilistic bias in its new learning process. We present how the effectiveness of each reuse episode is indicative of the novelty of the new task with respect to the previously solved ones in the policy library. In the paper we review Policy Reuse, and we introduce theoretical results that demonstrate that: (i) a Policy Library can be selectively and incrementally built while learning different problems; (ii) the Policy Library can be understood as a basis of the domain that represents its structure through a set of core policies; and (iii) given the basis of a domain, we can define a lower bound for its reuse gain. Copyright © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",,"Computational methods; Learning systems; Libraries; Probability; Problem solving; Reusability; Core policies; Lower bound; Policy library; Policy Reuse learner; Public policy","Fernández, F.; Computer Science Department, 5000 Forbes Avenue, Pittsburgh, PA 15213, United States; email: fernando@cs.cmu.edu",,,,,,,"English","ICAPS Proc. Sixteenth Int. Conf. Autom. Planning Scheduling",Conference Paper,"Final","",Scopus,2-s2.0-33746095398
"Schatzmann J., Weilhammer K., Stuttle M., Young S.","14120207400;14120319800;14120529900;7404515229;","A survey of statistical user simulation techniques for reinforcement- learning of dialogue management strategies",2006,"Knowledge Engineering Review","21","2",,"97","126",,214,"10.1017/S0269888906000944","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747607273&doi=10.1017%2fS0269888906000944&partnerID=40&md5=bd7af9cd93ecfe7089c5b221d2aad867","Cambridge University, Engineering Department, Trumpington Street, Cambridge CB21PZ, United Kingdom","Schatzmann, J., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB21PZ, United Kingdom; Weilhammer, K., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB21PZ, United Kingdom; Stuttle, M., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB21PZ, United Kingdom; Young, S., Cambridge University, Engineering Department, Trumpington Street, Cambridge CB21PZ, United Kingdom","Within the broad field of spoken dialogue systems, the application of machine-learning approaches to dialogue management strategy design is a rapidly growing research area. The main motivation is the hope of building systems that learn through trial-and-error interaction what constitutes a good dialogue strategy. Training of such systems could in theory be done using human users or using corpora of human-computer dialogue, but in practice the typically vast space of possible dialogue states and strategies cannot be explored without the use of automatic user simulation tools. This requirement for training statistical dialogue models has created an interesting new application area for predictive statistical user modelling and a variety of different techniques for simulating user behaviour have been presented in the literature ranging from simple Markov models to Bayesian networks. The development of reliable user simulation tools is critical to further progress on automatic dialogue management design but it holds many challenges, some of which have been encountered in other areas of current research on statistical user modelling, such as the problem of 'concept drift', the problem of combining content-based and collaboration-based modelling techniques, and user model evaluation. The latter topic is of particular interest, because simulation-based learning is currently one of the few applications of statistical user modelling that employs both direct 'accuracy-based' and indirect 'utility-based' evaluation techniques. In this paper, we briefly summarize the role of the dialogue manager in a spoken dialogue system, give a short introduction to reinforcement- learning of dialogue management strategies and review the literature on user modelling for simulation-based strategy learning. We further describe recent work on user model evaluation and discuss some of the current research issues in simulation-based learning from a user modelling perspective. © 2006. Cambridge University Press.",,"Computer simulation; Human computer interaction; Learning systems; Mathematical models; Problem solving; Statistical methods; Surveying; Automatic user; Building systems; Dialogue strategy; Management strategies; Reinforcement","Schatzmann, J.; Cambridge University, Trumpington Street, Cambridge CB21PZ, United Kingdom; email: js532@eng.cam.ac.uk",,,02698889,,KEREE,,"English","Knowl Eng Rev",Article,"Final","",Scopus,2-s2.0-33747607273
"Liu S., Henze G.P.","9245637300;7102253211;","Experimental analysis of simulated reinforcement learning control for active and passive building thermal storage inventory: Part 1. Theoretical foundation",2006,"Energy and Buildings","38","2",,"142","147",,75,"10.1016/j.enbuild.2005.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-27744574257&doi=10.1016%2fj.enbuild.2005.06.002&partnerID=40&md5=488a6a50abb91602bfc9f43d2b3c55c5","Architectural Engineering, University of Nebraska-Lincoln, 1110 South 67th Street, PKI 243, Omaha, NE 68182-0681, United States; Architectural Engineering, University of Nebraska-Lincoln, 1110 South 67th Street, PKI 203D, Omaha, NE 68182-0681, United States","Liu, S., Architectural Engineering, University of Nebraska-Lincoln, 1110 South 67th Street, PKI 243, Omaha, NE 68182-0681, United States; Henze, G.P., Architectural Engineering, University of Nebraska-Lincoln, 1110 South 67th Street, PKI 203D, Omaha, NE 68182-0681, United States","This paper is the first part of a two-part investigation of a novel approach to optimally control commercial building passive and active thermal storage inventory. The proposed building control approach is based on simulated reinforcement learning, which is a hybrid control scheme that combines features of model-based optimal control and model-free learning control. An experimental study was carried out to analyze the performance of a hybrid controller installed in a full-scale laboratory facility. The first part presents an overview of the project with an emphasis on the theoretical foundation. The motivation of the research will be introduced first, followed by a review of past work. A brief introduction of the theory is provided including classic reinforcement learning and its variation, so-called simulated reinforcement learning, which constitutes the basic architecture of the hybrid learning controller. A detailed discussion of the experimental results will be presented in the companion paper. © 2005 Elsevier B.V. All rights reserved.","Learning control; Load shifting; Optimal control; Reinforcement learning; Thermal Energy Storage (TES)","Computer simulation; Control equipment; Heat storage; Inventory control; Reinforcement; Thermal effects; Learning control; Load shifting; Optimal control; Reinforcement learning; Thermal energy storage (TES); Buildings","Liu, S.; Architectural Engineering, 1110 South 67th Street, PKI 243, Omaha, NE 68182-0681, United States; email: sliu@mail.unomaha.edu",,,03787788,,ENEBD,,"English","Energy Build.",Article,"Final","",Scopus,2-s2.0-27744574257
[No author name available],[No author id available],"Pacific Rim Knowledge Acquisition Workshop, PKAW 2006",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4303 LNAI",,,"1","257",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025177240&partnerID=40&md5=adf423bd24dabaca9eee38267732ef01",,"","The proceedings contain 25 papers. The special focus in this conference is on Knowledge Acquisition. The topics include: Visual knowledge annotation and management by using qualitative spatial information; a prototyping approach to ontology engineering; relating business process models to goal-oriented requirements models in KAOS; heuristic and rule-based knowledge acquisition; RFID tag based library marketing for improving patron services; extracting discriminative patterns from graph structured data using constrained search; evaluating learning algorithms with meta-learning schemes for a rule evaluation support method based on objective indices; training classifiers for unbalanced distribution and cost-sensitive domains with ROC analysis; intelligent decision support for medication review; a hybrid browsing mechanism using conceptual scales; knowledge representation for video assisted by domain-specific ontology; an ontological infrastructure for the semantic integration of clinical archetypes; improvement of air handling unit control performance using reinforcement learning; optimizing dissimilarity-based classifiers using a newly modified hausdorff distance; a new model for classifying DNA code inspired by neural networks and FSA; improvements on common vector approach using k-clustering method; the method for the unknown word classification; an ontology supported approach to learn term to concept mapping; building corporate knowledge through ontology integration; planning with domain rules based on state-independent activation sets; elicitation of non-functional requirement preference for actors of usecase from domain model; enhancing information retrieval using problem specific knowledge and acquiring innovation knowledge.",,,,"Richards D.Hoffmann A.Tsumoto S.Kang B.-H.","Springer Verlag",03029743,9783540689553,,,"English","Lect. Notes Comput. Sci.",Conference Review,"Final","",Scopus,2-s2.0-85025177240
"Bapi R.S., Chandrasekhar Pammi V.S., Miyapuram K.P., Ahmed","6603442804;12138912600;12139624500;57202453882;","Investigation of sequence processing: A cognitive and computational neuroscience perspective",2005,"Current Science","89","10",,"1690","1698",,16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-31644450927&partnerID=40&md5=461a1ba874ddf5bdbe652fc01d09185e","Department of Computer and Information Sciences, University of Hyderabad, Hyderabad 500 046, India; Centre for Cognitive Science, University of Hyderabad, Hyderabad 500 046, India","Bapi, R.S., Department of Computer and Information Sciences, University of Hyderabad, Hyderabad 500 046, India, Centre for Cognitive Science, University of Hyderabad, Hyderabad 500 046, India; Chandrasekhar Pammi, V.S., Department of Computer and Information Sciences, University of Hyderabad, Hyderabad 500 046, India; Miyapuram, K.P., Department of Computer and Information Sciences, University of Hyderabad, Hyderabad 500 046, India; Ahmed, Department of Computer and Information Sciences, University of Hyderabad, Hyderabad 500 046, India","Serial order processing or sequence processing underlies many human activities such as speech, language, skill learning, planning, problem-solving, etc. Investigating the neural bases of sequence processing enables us to understand serial order in cognition and also helps in building intelligent devices. In this article, we review various cognitive issues related to sequence processing with examples. Experimental results that give evidence for the involvement of various brain areas will be described. Finally, a theoretical approach based on statistical models and reinforcement learning paradigm is presented. These theoretical ideas are useful for studying sequence learning in a principled way. This article also suggests a two-way process diagram integrating experimentation (cognitive neuroscience) and theory/computational modelling (computational neuroscience). This integrated framework is useful not only in the present study of serial order, but also for understanding many cognitive processes.","Cognitive science; Computational modelling; Reinforcement learning; Sequence learning; Serial order",,"Bapi, R.S.; Department of Computer and Information Sciences, , Hyderabad 500 046, India; email: bapics@uohyd.ernet.in",,,00113891,,CUSCA,,"English","Curr. Sci.",Review,"Final","",Scopus,2-s2.0-31644450927
"Doya K., Uchibe E.","7004163287;6603720332;","The cyber rodent project: Exploration of adaptive mechanisms for self-preservation and self-reproduction",2005,"Adaptive Behavior","13","2",,"149","160",,46,"10.1177/105971230501300206","https://www.scopus.com/inward/record.uri?eid=2-s2.0-21344434798&doi=10.1177%2f105971230501300206&partnerID=40&md5=c1f37197a944bbe62ba9da24b60bfb9e","Initial Research Project, Okinawa Institute of Science and Technology, Japan; ATR Computational Neuroscience Laboratories, Japan; CREST, Japan Science and Technology Agency, Japan; Initial Research Project, Okinawa, Institute of Science and Technology, 12-22 Suzaki, Gushikawa, Okinawa 904-2234, Japan; Computational Neurobiology Department, ATR Computational Neuroscience Laboratories, Japan; Initial Research Project, Okinawa Institute of Science and Technology, 12-22 Suzaki, Gushikawa, Okinawa 904-2234, Japan","Doya, K., Initial Research Project, Okinawa Institute of Science and Technology, Japan, ATR Computational Neuroscience Laboratories, Japan, CREST, Japan Science and Technology Agency, Japan, Computational Neurobiology Department, ATR Computational Neuroscience Laboratories, Japan, Initial Research Project, Okinawa Institute of Science and Technology, 12-22 Suzaki, Gushikawa, Okinawa 904-2234, Japan; Uchibe, E., Initial Research Project, Okinawa Institute of Science and Technology, Japan, Initial Research Project, Okinawa, Institute of Science and Technology, 12-22 Suzaki, Gushikawa, Okinawa 904-2234, Japan","The aim of the Cyber Rodent project is to understand the origins of our reward and affective systems by building artificial agents that share the same intrinsic constraints as natural agents: Self-preservation and self-reproduction. A Cyber Rodent is a robot that can search for and recharge from battery packs on the floor and copy its programs to a nearby agent through its infrared communication port. This article reviews our research topics so far, including (1) evolution of neural controllers, (2) learning of foraging and mating behaviors, (3) evolution of learning architectures and meta-parameters, (4) simultaneous learning of multiple agents in a body, and (5) learning and evolution in a self-sustained colony. We discuss our future directions and expected contributions. Copyright © 2005 International Society for Adaptive Behavior.","Learning and evolution; Reinforcement learning; Reward function; Self-preservation and self-reproduction","Rodentia","Uchibe, E.; Initial Research Project, Okinawa, 12-22 Suzaki, Gushikawa, Okinawa 904-2234, Japan; email: uchibe@irp.oist.jp",,,10597123,,,,"English","Adapt. Behav.",Review,"Final","",Scopus,2-s2.0-21344434798
"Miller W.R., Meyers R.J., Hiller-Sturmhöfel S.","57211957670;7103328426;6603066940;","The community-reinforcement approach",2003,"Psychosocial Treatments",,,,"49","59",,1,"10.4324/9780203503508","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905543183&doi=10.4324%2f9780203503508&partnerID=40&md5=aa4e1b41304bbc73949a6047861ad5c6","Center on Alcoholism, Substance Abuse, and Addiction, University of New Mexico, Albuquerque, NM, United States; Alcohol Research and Health, United States","Miller, W.R., Center on Alcoholism, Substance Abuse, and Addiction, University of New Mexico, Albuquerque, NM, United States; Meyers, R.J., Center on Alcoholism, Substance Abuse, and Addiction, University of New Mexico, Albuquerque, NM, United States; Hiller-Sturmhöfel, S., Alcohol Research and Health, United States","The community-reinforcement approach (CRA) is an alcoholism treatment approach that aims to achieve abstinence by eliminating positive reinforcement for drinking and enhancing positive reinforcement for sobriety. CRA integrates several treatment components, including building the client's motivation to quit drinking, helping the client initiate sobriety, analyzing the client's drinking pattern, increasing positive reinforcement, learning new coping behaviors, and involving significant others in the recovery process. These components can be adjusted to the individual client's needs to achieve optimal treatment outcome. In addition, treatment outcome can be influenced by factors such as therapist style and initial treatment intensity. Several studies have provided evidence for CRA's effectiveness in achieving abstinence. Furthermore, CRA has been successfully integrated with a variety of other treatment approaches, such as family therapy and motivational interviewing, and has been tested in the treatment of other drug abuse. © 2004 by the American Academy of Addiction Psychiatry. All rights reserved.","AOD (alcohol and other drug) abstinence; AOD use pattern; AODD (alcohol and other drug dependence) recovery; AODU (alcohol and other drug use) treatment method; Cessation of AODU; Family therapy; Literature review; Motivation; Motivational interviewing; Professional client relations; Reinforcement; Spouse or significant other; Treatment outcome",,"Miller, W.R.; Center on Alcoholism, Substance Abuse, and Addiction, , Albuquerque, NM, United States",,"Brunner-Routledge",,0203503503; 9780203503508,,,"English","Psychosoc. Treat.",Book Chapter,"Final","",Scopus,2-s2.0-84905543183
"Baldassarre G.","57196314496;","Forward and bidirectional planning based on reinforcement learning and neural networks in a simulated robot.",2003,"Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)","2684",,,"179","200",,19,"10.1007/978-3-540-45002-3_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-23144433803&doi=10.1007%2f978-3-540-45002-3_11&partnerID=40&md5=d7f05a4720a3028e5cf433ef511cf213","National Research Council of Italy (ISTC-CNR) Viale Marx 15, United Kingdom Institute of Cognitive Sciences and Technologies, University of Essex, CO4 3SQ, Colchester, Rome, 00137, Italy","Baldassarre, G., National Research Council of Italy (ISTC-CNR) Viale Marx 15, United Kingdom Institute of Cognitive Sciences and Technologies, University of Essex, CO4 3SQ, Colchester, Rome, 00137, Italy","Building intelligent systems that are capable of learning, acting reac-tively and planning actions before their execution is a major goal of artificial intelligence. This paper presents two reactive and planning systems that contain important novelties with respect to previous neural-network planners and rein-forcement-learning based planners: (a) the introduction of a new component (.matcher.) allows both planners to execute genuine taskable planning (while previous reinforcement-learning based models have used planning only for speeding up learning); (b) the planners show for the first time that trained neu-ral-network models of the world can generate long prediction chains that have an interesting robustness with regards to noise; (c) two novel algorithms that generate chains of predictions in order to plan, and control the flows of infor-mation between the systems. different neural components, are presented; (d) one of the planners uses backward .predictions. to exploit the knowledge of the pursued goal; (e) the two systems presented nicely integrate reactive behav-ior and planning on the basis of a measure of .confidence. in action. The soundness and potentialities of the two reactive and planning systems are tested and compared with a simulated robot engaged in a stochastic path-finding task. The paper also presents an extensive literature review on the relevant issues. © 2003 Springer-Verlag Berlin Heidelberg.",,"Algorithms; Computer simulation; Dynamic programming; Learning systems; Multi agent systems; Neural networks; Numerical methods; Reinforcement; Algorithms; Artificial intelligence; Chains; Forecasting; Intelligent buildings; Intelligent systems; Learning systems; Planning; Robot programming; Stochastic systems; Bidirectional planning; Planning systems; Reinforcement learning; Simulated robots; Robots; Reinforcement learning; Bidirectional planning; Building intelligent systems; Literature reviews; Network planners; Prediction chains; Reactive behavior; Stochastic path finding; Trained neural networks","Baldassarre, G.; National Research Council of Italy (ISTC-CNR) Viale Marx 15, CO4 3SQ, Colchester, Italy; email: baldassarre@ip.rm.cnr.it","Butz M.V.Butz M.V.Sigaud O.Gerard P.","Springer Verlag",03029743,3540404295; 9783540404293,LNAIE,,"English","Lect Notes Artif Intell",Conference Paper,"Final","",Scopus,2-s2.0-23144433803
"Margoliash D.","7003716661;","Evaluating theories of bird song learning: Implications for future directions",2002,"Journal of Comparative Physiology A: Neuroethology, Sensory, Neural, and Behavioral Physiology","188","11-12",,"851","866",,47,"10.1007/s00359-002-0351-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037004141&doi=10.1007%2fs00359-002-0351-5&partnerID=40&md5=108df370456333879ab070281d9bf7d8","Dept. of Organismal Biol./Anatomy, University of Chicago, 1027 E. 57th St., Chicago, IL 60637, United States","Margoliash, D., Dept. of Organismal Biol./Anatomy, University of Chicago, 1027 E. 57th St., Chicago, IL 60637, United States","Studies of birdsong learning have stimulated extensive hypotheses at all levels of behavioral and physiological organization. This hypothesis building is valuable for the field and is consistent with the remarkable range of issues that can be rigorously addressed in this system. The traditional instructional (template) theory of song learning has been challenged on multiple fronts, especially at a behavioral level by evidence consistent with selectional hypotheses. In this review I highlight the caveats associated with these theories to better define the limits of our knowledge and identify important experiments for the future. The sites and representational forms of the various conceptual entities posited by the template theory are unknown. The distinction between instruction and selection in vocal learning is not well established at a mechanistic level. There is as yet insufficient neurophysiological data to choose between competing mechanisms of error-driven learning and reinforcement learning. Both may obtain for vocal learning. The possible role of sleep in acoustic or procedural memory consolidation, while supported by some physiological observations, does not yet have support in the behavioral literature. The remarkable expansion of knowledge in the past 20 years and the recent development of new technologies for physiological and behavioral experiments should permit direct tests of these theories in the coming decade.","Error-driven and reinforcement learning; Instruction and selection; Sleep and replay; Template theory","animal; animal communication; biological model; bird; brain; conference paper; evaluation; feedback system; growth, development and aging; histology; learning; nerve cell plasticity; nerve tract; nervous system; physiology; psychomotor performance; reinforcement; sleep; sound; synapse; time; vocalization; Animal Communication; Animals; Birds; Brain; Evaluation Studies; Feedback; Learning; Models, Biological; Nervous System; Neural Pathways; Neuronal Plasticity; Psychomotor Performance; Reinforcement (Psychology); Sleep; Sound; Synapses; Time Factors; Vocalization, Animal","Margoliash, D.; Dept. of Organismal Biol./Anatomy, 1027 E. 57th St., Chicago, IL 60637, United States; email: dan@bigbird.uchicago.edu",,,03407594,,JCPAD,"12471486","English","J. Comp. Physiol. A Neuroethol. Sens. Neural. Behav. Physiol.",Conference Paper,"Final","",Scopus,2-s2.0-0037004141
"Miller W.R., Meyers R.J., Hiller-Sturmhöfel S.","57211957670;7103328426;6603066940;","The community-reinforcement approach",1999,"Alcohol Research and Health","23","2",,"116","120",,69,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033291778&partnerID=40&md5=e056ae215b0893f878c923aa73e0d715","Ctr. Alcoholism, Substance Abuse, A., University of New Mexico, Albuquerque, NM, United States","Miller, W.R., Ctr. Alcoholism, Substance Abuse, A., University of New Mexico, Albuquerque, NM, United States; Meyers, R.J., Ctr. Alcoholism, Substance Abuse, A., University of New Mexico, Albuquerque, NM, United States; Hiller-Sturmhöfel, S.","The community-reinforcement approach (CRA) is an alcoholism treatment approach that aims to achieve abstinence by eliminating positive reinforcement for drinking and enhancing positive reinforcement for sobriety. CRA integrates several treatment components, including building the client's motivation to quit drinking, helping the client initiate sobriety, analyzing the client's drinking pattern, increasing positive reinforcement, learning new coping behaviors, and involving significant others in the recovery process. These components can be adjusted to the individual client's needs to achieve optimal treatment outcome. In addition, treatment outcome can be influenced by factors such as therapist style and initial treatment intensity. Several studies have provided evidence for CRA's effectiveness in achieving abstinence. Furthermore, CRA has been successfully integrated with a variety of other treatment approaches, such as family therapy and motivational interviewing, and has been tested in the treatment of other drug abuse.","AOD (alcohol and other drug) abstinence; AOD use pattern; AODD (alcohol and other drug dependence) recovery; AODU (alcohol and other drug use) treatment method; Cessation of AODU; Motivation; Professional client relations; Reinforcement; Treatment outcome","adaptive behavior; alcoholism; community care; human; motivation; psychological aspect; reinforcement; review; Adaptation, Psychological; Alcoholism; Community Networks; Humans; Motivation; Reinforcement (Psychology)","Miller, W.R.; Ctr. Alcoholism, Substance Abuse, A., , Albuquerque, NM, United States",,,0090838X,,,"2000137269","English","Alcohol Res. Health",Article,"Final","",Scopus,2-s2.0-0033291778
